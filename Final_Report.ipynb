{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import Gamma\n",
    "from torch.nn import functional as F\n",
    "import torch.tensor as Tensor\n",
    "import torch.nn.init as init\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from tqdm import tqdm_notebook\n",
    "from multiprocessing import Pool\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from rangerlars import RangerLars\n",
    "\n",
    "\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Transactions Loading\n",
    "We firstly load wire transactions to extract usefull features.\n",
    "Here we generate 6 features, listed as folllows.\n",
    "+ Transact out counts in one month is large than 500,000\n",
    "+ WireTrans Out counts in one month\n",
    "+ Transact in total amount per month is large than 1,000,000\n",
    "+ Total WireTrans Out in one month\n",
    "+ Total WireTrans Out times in one month\n",
    "+ Average WireTrans Out in one month\n",
    "At last, we store the edges in the form of `pandas` per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustInfo = pd.read_csv('../data/customerinformation.csv')\n",
    "CustInfo['open_date'] = pd.to_datetime(CustInfo.Open_Date)\n",
    "\n",
    "WireTrans = pd.read_csv('../data/new_wire.csv', index_col=0)\n",
    "WireTrans['trans_date'] = pd.to_datetime(WireTrans.trandt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete Non-IBW Customer_Segment_Code customerno\n",
    "new_wire = WireTrans[WireTrans.customerno.isin(CustInfo[CustInfo.Customer_Segment_Code != 'IBW']['customerno'].values)]\n",
    "\n",
    "# According Trf_Direction to build new feature  \n",
    "new_wire['bnf_cust'] = new_wire.apply(lambda x: x['customerno'] if x['Trf_Direction'] == 'I' else np.nan ,axis=1)\n",
    "\n",
    "new_wire['org_cust'] = new_wire.apply(lambda x: x['customerno'] if x['Trf_Direction'] == 'O' else np.nan ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateYM:\n",
    "    def __init__(self, year, month):\n",
    "        self.year = year\n",
    "        self.month = month - 1    # 0 ~ 11, from Jan to Dec\n",
    "        \n",
    "    def export_tuple(self):\n",
    "        return (self.year, self.month+1)\n",
    "    \n",
    "    def add_year(self, y):\n",
    "        self.year += y\n",
    "        \n",
    "    def substract_year(self, y):\n",
    "        self.year -= y\n",
    "        \n",
    "    def add_month(self, m):\n",
    "        self.month += m\n",
    "        self.year += math.floor(self.month / 12)\n",
    "        self.month = self.month % 12\n",
    "        \n",
    "    def subtract_month(self, m):\n",
    "        self.month -= m\n",
    "        tmp_year = math.floor(self.month / 12)\n",
    "        self.year += tmp_year\n",
    "        self.month += -tmp_year * 12\n",
    "        \n",
    "    def is_larger_than(self, ym):\n",
    "        return self.year*12 + self.month > ym.year*12 + ym.month\n",
    "    \n",
    "    def is_smaller_than(slef, ym):\n",
    "        return self.year*12 + self.month < ym.year*12 + ym.month\n",
    "    \n",
    "    def is_equal(self, ym):\n",
    "        return self.year*12 + self.month == ym.year*12 + ym.month\n",
    "\n",
    "    \n",
    "def list_date_tuples(from_date, to_date):\n",
    "    ret = []\n",
    "    tmp_date = DateYM(*from_date.export_tuple())\n",
    "    while not tmp_date.is_larger_than(to_date):\n",
    "        ret.append(tmp_date.export_tuple())\n",
    "        tmp_date.add_month(1)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def fetch_data_by_month(date_ym, trans_offset=6):\n",
    "    year, month = date_ym\n",
    "    \n",
    "    from_date = pd.to_datetime(\"{}/{}/{}\".format(month, 1, year))\n",
    "    to_date = from_date + pd.DateOffset(months=1)\n",
    "    offset_date = from_date - pd.DateOffset(months=trans_offset)\n",
    "    \n",
    "    # Get view: WireTrans\n",
    "    view_wiretrans = WireTrans[(WireTrans.trans_date > offset_date) & \n",
    "                               (WireTrans.trans_date < to_date)] \n",
    "\n",
    "    # Get view: CustInfo\n",
    "    view_customer = CustInfo[CustInfo.open_date < to_date]\n",
    "    \n",
    "    # Attach label onto CustInfo\n",
    "    target_list = SARCase[(SARCase.Status_SAR == 4) & \n",
    "                          (SARCase.created_date > from_date) & \n",
    "                          (SARCase.created_date < to_date)]['customerno'].unique()\n",
    "\n",
    "    view_customer['label'] = view_customer.apply(lambda x: 1 if x['customerno'] in target_list else 0, axis=1)\n",
    "    view_customer  = view_customer.set_index('customerno')\n",
    "    view_wiretrans = view_wiretrans.set_index('customerno')\n",
    "    return view_wiretrans, view_customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date_ym = DateYM(2017, 8)\n",
    "to_date_ym = DateYM(2019, 6)\n",
    "trans_out = 500000\n",
    "trans_in  = 1000000\n",
    "\n",
    "list_date_seq = list_date_tuples(from_date_ym, to_date_ym)\n",
    "projectors_shape = dict()\n",
    "for i, date_ym in zip(range(len(list_date_seq)), list_date_seq):\n",
    "    print(\"Processing the data in {}\".format(date_ym))\n",
    "    \n",
    "    view_wiretrans, view_customer = fetch_data_by_month(date_ym, trans_offset=1)\n",
    "    view_wiretrans['edge_group'] = view_wiretrans.apply(lambda x: \"{}_{}\".format(x.org_cust, x.bnf_cust), axis=1)\n",
    "    \n",
    "    ## Transact out counts in one month is large than 500,000\n",
    "    Large_TransOut_Count = view_wiretrans[view_wiretrans.WIRE_AMTOT > trans_out].edge_group.value_counts().rename('Large_TransOut_Count')\n",
    "    \n",
    "    ##　WireTrans Out counts in one month\n",
    "    TransOut_Count = view_wiretrans[view_wiretrans.WIRE_AMTOT > 0].edge_group.value_counts().rename('TransOut_Count')\n",
    "    \n",
    "    ##　Transact in total amount per month is large than 1,000,000\n",
    "    Total_Large_TransIn = view_wiretrans[view_wiretrans.WIRE_AMTIN > trans_in].groupby('edge_group').sum()['WIRE_AMTIN'].rename('Total_Large_TransIn')\n",
    "    \n",
    "    ## Total WireTrans Out in one month\n",
    "    Total_WireTrans = view_wiretrans[view_wiretrans.WIRE_AMTOT > 0].groupby('edge_group').sum()['WIRE_AMTOT'].rename('Total_WireTrans')\n",
    "    \n",
    "    ## Total WireTrans Out times in one month\n",
    "    Total_WireTrans_Times = view_wiretrans[view_wiretrans.WIRE_AMTOT > 0].groupby('edge_group').size().rename('Total_WireTrans_Times')\n",
    "    \n",
    "    ## Average WireTrans Out in one month\n",
    "    Average_WireTrans = (Total_WireTrans/Total_WireTrans_Times).rename('Average_WireTrans')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## WireTrans in more than 8,000, and total amount is more than 60,000,000\n",
    "    WireTransIn_8000 = view_wiretrans[view_wiretrans.WIRE_AMTIN > 8000].groupby('edge_group').sum()\n",
    "    WireTransIn_8000 = WireTransIn_8000['WIRE_AMTIN'][WireTransIn_8000['WIRE_AMTIN']>60000000].to_frame()\n",
    "    WireTransIn_8000['WireTransIn_8000'] = 1\n",
    "    WireTransIn_8000 = WireTransIn_8000['WireTransIn_8000']\n",
    "    \n",
    "    ## 9 months\n",
    "    view_wiretrans, view_customer = fetch_data_by_month(date_ym, trans_offset=9)\n",
    "    view_wiretrans['edge_group'] = view_wiretrans.apply(lambda x: \"{}_{}\".format(x.org_cust, x.bnf_cust), axis=1)\n",
    "    ## WireTrans more than 8,000 in nine months\n",
    "    WireTrans_Out_9mon = view_wiretrans[view_wiretrans.WIRE_AMTOT > 8000].groupby('edge_group').sum()['WIRE_AMTOT'].rename('WireTrans_Out_9mon')\n",
    "    \n",
    "    ## Output\n",
    "    output = pd.concat([Large_TransOut_Count, \n",
    "                        TransOut_Count, \n",
    "                        Total_Large_TransIn, \n",
    "                        Total_WireTrans, \n",
    "                        Total_WireTrans_Times, \n",
    "                        Average_WireTrans,\n",
    "                        WireTransIn_8000,\n",
    "                        WireTrans_Out_9mon], axis=1).fillna(0)\n",
    "    output = pd.concat([pd.DataFrame([idx.split('_') for idx in output.index], index=output.index), output], axis=1)\n",
    "    output.to_csv(\"./Edge_Attribute/Edge_attribue_{}-{}.csv\".format(date_ym[0], date_ym[1]), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Conversion from `EdgeAttibute` to `EdgeEmbeddings` via `GammaVAE`\n",
    "6 features are too many to run the embeddings extraction.\n",
    "Thus, we run a `GammaVAE` to encode the 6 features into 3-dimensional embeddings.\n",
    "Notice that the classic **VAE** is assmumed to be Gaussian distributed, which could be negative. However, the edge should be non-negative according to our defintion.\n",
    "In light of this, the AE model we use should be non-negative. Intuitively, GammaVAE is Gamma distributed, which is non-negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Paramters Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelConfig = namedtuple('ModelConfig', ['input_dims', 'latent_dims', 'hidden_dims', 'gamma_shape', 'prior_shape', 'prior_rate', 'prior_weight'])\n",
    "model_config = ModelConfig(\n",
    "    input_dims = 8,\n",
    "    latent_dims = 3,\n",
    "    hidden_dims = [16, 8, 5],\n",
    "    gamma_shape = 8.,\n",
    "    prior_shape = 2.0,\n",
    "    prior_rate = 1.,\n",
    "    prior_weight = 0.001,\n",
    ")\n",
    "\n",
    "TrainConfig = namedtuple('TrainConfig', ['training_epochs', 'batch_size', 'learning_rate'])\n",
    "train_config = TrainConfig(\n",
    "    training_epochs = 200,\n",
    "    batch_size = 1000,\n",
    "    learning_rate = 1e-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Definition of `GammaVAE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GammaVAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: list = None,\n",
    "                 gamma_shape: float = 8.,\n",
    "                 prior_shape: float = 2.0,\n",
    "                 prior_rate: float = 1.,\n",
    "                 prior_weight: float = 0.1,\n",
    "                 **kwargs) -> None:\n",
    "        super(GammaVAE, self).__init__()\n",
    "        \n",
    "        #\n",
    "        # Parameters setting\n",
    "        # --------------------------------------------------------------------------------------------------------------\n",
    "        self.input_dim = in_channels\n",
    "        self.latent_dim = latent_dim\n",
    "        self.B = gamma_shape\n",
    "        self.prior_alpha = torch.tensor([prior_shape])\n",
    "        self.prior_beta = torch.tensor([prior_rate])\n",
    "        self.prior_weight = prior_weight\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "        \n",
    "        #\n",
    "        # Build Encoder\n",
    "        # --------------------------------------------------------------------------------------------------------------\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_channels, out_features=h_dim),\n",
    "                    nn.BatchNorm1d(h_dim),\n",
    "                    nn.ELU(),\n",
    "                ))\n",
    "            in_channels = h_dim\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Sequential(nn.Linear(hidden_dims[-1], latent_dim),\n",
    "                                   nn.Softmax())\n",
    "        self.fc_var = nn.Sequential(nn.Linear(hidden_dims[-1], latent_dim),\n",
    "                                    nn.Softmax())\n",
    "\n",
    "        #\n",
    "        # Build Decoder\n",
    "        # --------------------------------------------------------------------------------------------------------------\n",
    "        modules = []\n",
    "        self.decoder_input = nn.Sequential(nn.Linear(latent_dim, hidden_dims[-1]))\n",
    "        hidden_dims = hidden_dims[::-1]\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_features=hidden_dims[i], out_features=hidden_dims[i + 1]),\n",
    "                    nn.BatchNorm1d(hidden_dims[i + 1]),\n",
    "                    nn.ELU(),\n",
    "                ))\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_dims[-1], out_features=self.input_dim),\n",
    "            nn.BatchNorm1d(self.input_dim),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                init_(m)\n",
    "\n",
    "    def encode(self, input: Tensor):\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        result = self.encoder(input)\n",
    "\n",
    "        # Split the result into mu and var components of the latent Gaussian distribution\n",
    "        alpha = self.fc_mu(result)\n",
    "        beta = self.fc_var(result)\n",
    "\n",
    "        return [alpha, beta]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        result = self.decoder_input(z)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, alpha: Tensor, beta: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterize the Gamma distribution by the shape augmentation trick.\n",
    "        Reference:\n",
    "        [1] https://arxiv.org/pdf/1610.05683.pdf\n",
    "\n",
    "        :param alpha: (Tensor) Shape parameter of the latent Gamma\n",
    "        :param beta: (Tensor) Rate parameter of the latent Gamma\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Sample from Gamma to guarantee acceptance\n",
    "        alpha_ = alpha.clone().detach()\n",
    "        z_hat = Gamma(alpha_ + self.B, torch.ones_like(alpha_)).sample()\n",
    "\n",
    "        # Compute the eps ~ N(0,1) that produces z_hat\n",
    "        eps = self.inv_h_func(alpha + self.B , z_hat)\n",
    "        z = self.h_func(alpha + self.B, eps)\n",
    "\n",
    "        # When beta != 1, scale by beta\n",
    "        return z / beta\n",
    "\n",
    "    @staticmethod\n",
    "    def h_func(alpha: Tensor, eps: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterize a sample eps ~ N(0, 1) so that h(z) ~ Gamma(alpha, 1)\n",
    "        :param alpha: (Tensor) Shape parameter\n",
    "        :param eps: (Tensor) Random sample to reparameterize\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "\n",
    "        z = (alpha - 1./3.) * (1 + eps / torch.sqrt(9. * alpha - 3.))**3\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def inv_h_func(alpha: Tensor, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Inverse reparameterize the given z into eps.\n",
    "        :param alpha: (Tensor)\n",
    "        :param z: (Tensor)\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        eps = torch.sqrt(9. * alpha - 3.) * ((z / (alpha - 1./3.))**(1. / 3.) - 1.)\n",
    "        return eps\n",
    "\n",
    "    @staticmethod\n",
    "    def I_function(a, b, c, d):\n",
    "        return - c * d / a - b * torch.log(a) - torch.lgamma(b) + (b - 1) * (torch.digamma(d) + torch.log(c))\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> Tensor:\n",
    "        alpha, beta = self.encode(input)\n",
    "        z = self.reparameterize(alpha, beta)\n",
    "        return [self.decode(z), input, alpha, beta]\n",
    "\n",
    "    def vae_gamma_kl_loss(self, a, b, c, d):\n",
    "        \"\"\"\n",
    "        https://stats.stackexchange.com/questions/11646/kullback-leibler-divergence-between-two-gamma-distributions\n",
    "        b and d are Gamma shape parameters and\n",
    "        a and c are scale parameters.\n",
    "        (All, therefore, must be positive.)\n",
    "        \"\"\"\n",
    "\n",
    "        a = 1 / a\n",
    "        c = 1 / c\n",
    "        losses = self.I_function(c, d, c, d) - self.I_function(a, b, c, d)\n",
    "        return torch.sum(losses, dim=1)\n",
    "\n",
    "    def loss_function(self, *args, **kwargs) -> dict:\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        alpha = args[2]\n",
    "        beta = args[3]\n",
    "\n",
    "        curr_device = input.device\n",
    "        recons_loss = torch.mean(F.mse_loss(recons, input, reduction='none'), dim=(1,))\n",
    "\n",
    "        self.prior_alpha = self.prior_alpha.to(curr_device)\n",
    "        self.prior_beta = self.prior_beta.to(curr_device)\n",
    "\n",
    "        kld_loss = self.vae_gamma_kl_loss(alpha, beta, self.prior_alpha, self.prior_beta)\n",
    "\n",
    "        loss = (1 - self.prior_weight) * recons_loss + self.prior_weight * kld_loss\n",
    "        loss = torch.mean(loss, dim = 0)\n",
    "        # print(loss, recons_loss, kld_loss)\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def sample(self, num_samples:int, current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the modelSay\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = Gamma(self.prior_alpha, self.prior_beta).sample((num_samples, self.latent_dim))\n",
    "        z = z.squeeze().to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]\n",
    "\n",
    "\n",
    "def init_(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        init.orthogonal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "        m.weight.data.fill_(1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Data Loading\n",
    "Load data to train **GamaVAE**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = glob.glob('./Edge_Attribute/*.csv')\n",
    "dfs = [pd.read_csv(url, encoding = \"ISO-8859-1\", engine='python') for url in urls]\n",
    "df_train = pd.concat(dfs[:16])\n",
    "df_test  = pd.concat(dfs[16:])\n",
    "\n",
    "edge_attr_train = df_train.iloc[:,2:]\n",
    "edge_attr_test  = df_test.iloc[:,2:]\n",
    "edge_attr_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr_train = np.log(edge_attr_train + 1.)\n",
    "edge_attr_test = np.log(edge_attr_test + 1.)\n",
    "edge_attr_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "edge_attr_train_scaled = scaler.fit_transform(edge_attr_train)\n",
    "edge_attr_test_scaled  = scaler.transform(edge_attr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr_train_scaled = pd.DataFrame(edge_attr_train_scaled)\n",
    "edge_attr_train_scaled = torch.tensor(edge_attr_train_scaled.values, dtype=torch.float32).cuda()\n",
    "\n",
    "edge_attr_test_scaled = pd.DataFrame(edge_attr_test_scaled)\n",
    "edge_attr_test_scaled = torch.tensor(edge_attr_test_scaled.values, dtype=torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Model & Optimizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = GammaVAE(in_channels=model_config.input_dims,\n",
    "               latent_dim=model_config.latent_dims,\n",
    "               hidden_dims=model_config.hidden_dims,\n",
    "               gamma_shape=model_config.gamma_shape,\n",
    "               prior_shape=model_config.prior_shape,\n",
    "               prior_rate=model_config.prior_rate,\n",
    "               prior_weight=model_config.prior_weight).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=train_config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice = torch.tensor(random.sample(range(edge_attr_train_scaled.shape[0]), train_config.batch_size))\n",
    "batch_xs = edge_attr_train_scaled[indice]\n",
    "x_output, x_input, x_alpha, x_beta = vae(batch_xs)\n",
    "loss_dict = vae.loss_function(x_output, x_input, x_alpha, x_beta)\n",
    "train_loss_ini = loss_dict['loss']\n",
    "\n",
    "indice = torch.tensor(random.sample(range(edge_attr_test_scaled.shape[0]), train_config.batch_size))\n",
    "batch_xs = edge_attr_test_scaled[indice]\n",
    "x_output, x_input, x_alpha, x_beta = vae(batch_xs)\n",
    "loss_dict = vae.loss_function(x_output, x_input, x_alpha, x_beta)\n",
    "test_loss_ini = loss_dict['loss']\n",
    "\n",
    "print(\"Epoch:\", '%04d' % (0), \n",
    "      \"Training loss=\", \"{:.9f}\".format(train_loss_ini), \n",
    "      \"Testing loss=\", \"{:.9f}\".format(test_loss_ini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(train_config.training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(edge_attr_train_scaled.shape[0] / train_config.batch_size)\n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    avg_train_cost = 0.\n",
    "    avg_test_cost = 0.\n",
    "    for i in range(total_batch):\n",
    "        \n",
    "        ## train\n",
    "        indice = torch.tensor(random.sample(range(edge_attr_train_scaled.shape[0]), train_config.batch_size))\n",
    "        batch_xs = edge_attr_train_scaled[indice]\n",
    "\n",
    "        x_output, x_input, x_alpha, x_beta = vae(batch_xs)\n",
    "        loss_dict = vae.loss_function(x_output, x_input, x_alpha, x_beta)\n",
    "        train_loss.append(loss_dict['loss'] / edge_attr_train_scaled.shape[0] * train_config.batch_size)\n",
    "        avg_train_cost += train_loss[-1]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss[-1].backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ## valid\n",
    "        indice = torch.tensor(random.sample(range(edge_attr_test_scaled.shape[0]), train_config.batch_size))\n",
    "        batch_xs = edge_attr_test_scaled[indice]\n",
    "\n",
    "        x_output, x_input, x_alpha, x_beta = vae(batch_xs)\n",
    "        loss_dict = vae.loss_function(x_output, x_input, x_alpha, x_beta)\n",
    "        test_loss.append(loss_dict['loss'] / edge_attr_train_scaled.shape[0] * train_config.batch_size)\n",
    "        avg_test_cost += test_loss[-1]\n",
    "     \n",
    "    # Display logs per epoch step    \n",
    "    print(\"Epoch:\", '%04d' % (epoch+1), \n",
    "          \"Training loss=\", \"{:.9f}\".format(avg_train_cost), \"Testing loss=\", \"{:.9f}\".format(test_loss[-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Generating `EdgeEmbeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_output, train_x_input, train_x_alpha, train_x_beta = vae(edge_attr_train_scaled)\n",
    "train_mean = train_x_alpha / train_x_beta\n",
    "train_mean[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_output, test_x_input, test_x_alpha, test_x_beta = vae(edge_attr_test_scaled)\n",
    "test_mean = test_x_alpha / test_x_beta\n",
    "test_mean[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAE_res = np.concatenate((train_mean.cpu().data.numpy(), test_mean.cpu().data.numpy()), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "index = 0\n",
    "for url in urls:\n",
    "    date = url.split('_')[-1].split('.')[0]\n",
    "    print (date, dfs[index].shape[0])\n",
    "    tmp_df =  pd.concat([pd.DataFrame(dfs[index].iloc[:,:2].values), \n",
    "                         pd.DataFrame(VAE_res[count:count+dfs[index].shape[0],:])], axis=1)\n",
    "    tmp_df.to_csv('./Edge_Attribute/Edge_Embedd_{}.csv'.format(date), header=False)\n",
    "    count += dfs[index].shape[0]\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Graph Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Parameters Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "latent_dim = 300\n",
    "epochs = 600\n",
    "lr = 100\n",
    "lambda_V = 1e-5\n",
    "lambda_w = 1e-4\n",
    "param_alpha = 1\n",
    "focal_alpha = 0.1\n",
    "focal_gamma = 1.5\n",
    "edge_attrs = ['dim_1', 'dim_2', 'dim_3']\n",
    "L = 20\n",
    "\n",
    "# Graph fetching settings\n",
    "weight_type = 'embed'  # [weight_none, weight_log, count_larger8000]\n",
    "offset = 0\n",
    "\n",
    "# To store all the infomation of dynamic graphs\n",
    "dyngraph_info = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Fetch data by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date_ym = DateYM(2018, 1)\n",
    "to_date_ym = DateYM(2019, 6)\n",
    "list_date_seq = list_date_tuples(from_date_ym, to_date_ym)\n",
    "seq_length = len(list_date_seq)\n",
    "print(list_date_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Load Edge Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_embed(embed_path, groupby_method='sum'):\n",
    "    df = pd.read_csv(embed_path)\n",
    "    df.columns = ['index', 'from_vertex', 'to_vertex', 'dim_1', 'dim_2', 'dim_3']\n",
    "    df['vertex_index'] = df.apply(lambda x: str(set([x['from_vertex'], x['to_vertex']])), axis=1)\n",
    "    if groupby_method == 'sum':\n",
    "        df_new = df.groupby('vertex_index').sum().reset_index().reset_index()\n",
    "    else:\n",
    "        df_new = df.groupby('vertex_index').mean().reset_index().reset_index()\n",
    "    df_new['from_vertex'] = df_new.apply(lambda x: x['vertex_index'][1:-1].split(',')[0][1:-1], axis=1)\n",
    "    df_new['to_vertex'] = df_new.apply(lambda x: x['vertex_index'][1:-1].split(',')[1][2:-1], axis=1)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, month in list_date_seq:\n",
    "    edge_embed_path = './Edge_Attribute/Edge_Embedd_{}-{}.csv'.format(year, month)\n",
    "    edge_embed = get_edge_embed(edge_embed_path)\n",
    "    dyngraph_info[(year, month)]['edge_embed'] = edge_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Vertex Indexing\n",
    "To construct an indexing mapping between `entry_index` and `custimer_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, month in list_date_seq:\n",
    "    vertex_ids_t = set.union(set(dyngraph_info[(year, month)]['edge_embed']['from_vertex']),\n",
    "                             set(dyngraph_info[(year, month)]['edge_embed']['to_vertex']))\n",
    "    dict_int2key = dict(map(lambda x, y: (x, y), range(len(vertex_ids_t)), vertex_ids_t))\n",
    "    dict_key2int = dict(map(lambda x, y: (y, x), range(len(vertex_ids_t)), vertex_ids_t))\n",
    "    dyngraph_info[(year, month)]['int2key'] = dict_int2key\n",
    "    dyngraph_info[(year, month)]['key2int'] = dict_key2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, month in list_date_seq:\n",
    "    with open('../data/index_mapping/int2key_{}{}.pickle'.format(str(year), str(month).zfill(2)), 'wb') as handle:\n",
    "        pickle.dump(dyngraph_info[(year, month)]['int2key'], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open('../data/index_mapping/key2int_{}{}.pickle'.format(str(year), str(month).zfill(2)), 'wb') as handle:\n",
    "        pickle.dump(dyngraph_info[(year, month)]['key2int'], handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Load Ground Truth Labels\n",
    "- `FILTER`: required for monitoring \n",
    "- `announce`: suspicious account for AML\n",
    "- `Y_SAR`: AML account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, month in list_date_seq:\n",
    "    ctbc_path = '../data/aml_all_{}{}_y.csv'.format(str(year)[2:], str(month).zfill(2))\n",
    "    ctbc_data = pd.read_csv(ctbc_path)\n",
    "    \n",
    "    dict_key2int_t = dyngraph_info[(year, month)]['key2int']\n",
    "    pos_index = [dict_key2int_t[x] for x in list(ctbc_data[ctbc_data['Y_SAR'] == 1]['customerno']) if x in dict_key2int_t]\n",
    "    y_t = np.zeros((len(dict_key2int_t), 1))\n",
    "    y_t[pos_index, 0] = 1\n",
    "    dyngraph_info[(year, month)]['y'] = y_t\n",
    "\n",
    "    print(\"({}, {}): # of FILTER = {}, # of announce = {}, # of Y_SAR = {}, # of PosVertex = {}\".\n",
    "          format(year, month, \n",
    "                 sum(ctbc_data['FILTER']), sum(ctbc_data['announce']), \n",
    "                 sum(ctbc_data['Y_SAR']), int(sum(y_t))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Load Graph by Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, month in list_date_seq:\n",
    "    edge_embed_t = dyngraph_info[(year, month)]['edge_embed']\n",
    "    key2int_t = dyngraph_info[(year, month)]['key2int']\n",
    "    int2key_t = dyngraph_info[(year, month)]['int2key']\n",
    "    G_t = nx.from_pandas_edgelist(edge_embed_t, 'from_vertex', 'to_vertex', \n",
    "                                  edge_attr=edge_attrs,\n",
    "                                  create_using=nx.MultiGraph())\n",
    "    G_t = nx.relabel_nodes(G_t, key2int_t)\n",
    "    L_t = nx.normalized_laplacian_matrix(G_t, nodelist=int2key_t)\n",
    "    A_t = nx.to_scipy_sparse_matrix(G_t, nodelist=int2key_t)\n",
    "    A_ts = [nx.to_scipy_sparse_matrix(G_t, weight=attr, nodelist=int2key_t) for attr in edge_attrs]\n",
    "    dyngraph_info[(year, month)]['graph_laplacian'] = L_t\n",
    "    dyngraph_info[(year, month)]['graph_adjacent'] = A_ts\n",
    "    print('({}, {}): Finished graph constructuion, |V| = {}'.format(year, month, A_t.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Embedding Projection on a Dynamic graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if len(x.shape) > 0:\n",
    "        x[x < -5] = -5.\n",
    "    else:\n",
    "        x = -5. if x < -5. else x\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(input_pkg):\n",
    "    t, hetergraph = input_pkg\n",
    "    L_t, A_ts = hetergraph\n",
    "    date_ym = list_date_seq[t]\n",
    "    \n",
    "    key2int_t = dyngraph_info[date_ym]['key2int']\n",
    "    int2key_t = dyngraph_info[date_ym]['int2key']\n",
    "    \n",
    "    y_t = dyngraph_info[date_ym]['y']\n",
    "    V_t = dyngraph_info[date_ym]['vertex_embed']\n",
    "    w1 = classifiers['w1']\n",
    "    w2 = classifiers['w2']\n",
    "    we = classifiers['we']\n",
    "    theta = classifiers['theta']\n",
    "    \n",
    "    A_t = scipy.sparse.csr_matrix(A_ts[0].shape)\n",
    "    for index_attr in range(we.shape[0]):\n",
    "        tmpA_t = A_ts[index_attr]\n",
    "        A_t += sigmoid(we[index_attr,0]) * tmpA_t\n",
    "        \n",
    "    if (t + 1) < seq_length:\n",
    "        date_ym_plus = list_date_seq[t + 1]\n",
    "        \n",
    "        key2int_t_plus = dyngraph_info[date_ym_plus]['key2int']\n",
    "        int2key_t_plus = dyngraph_info[date_ym_plus]['int2key']\n",
    "        index_t_2_t_plus = [key2int_t_plus[x] if x in key2int_t_plus else -1 for x in int2key_t]\n",
    "        \n",
    "        # Get V_t_plus aligned in V_t index order\n",
    "        V_t_plus_orig = dyngraph_info[date_ym_plus]['vertex_embed']\n",
    "        V_t_plus_orig = np.append(V_t_plus_orig, np.zeros((1, V_t_plus_orig.shape[1])), axis=0)\n",
    "        V_t_plus = V_t_plus_orig[index_t_2_t_plus]\n",
    "        \n",
    "        # Get y_t_plus aligned in y_t index order\n",
    "        y_t_plus_orig = dyngraph_info[date_ym_plus]['y']\n",
    "        y_t_plus_orig = np.append(y_t_plus_orig, np.zeros((1,1)), axis=0)\n",
    "        y_t_plus = y_t_plus_orig[index_t_2_t_plus]\n",
    "    else:\n",
    "        V_t_plus = None\n",
    "        y_t_plus = None\n",
    "        key2int_t_plus = None\n",
    "        int2key_t_plus = None\n",
    "\n",
    "    if t > 0:\n",
    "        date_ym_minus = list_date_seq[t - 1]\n",
    "        \n",
    "        key2int_t_minus = dyngraph_info[date_ym_minus]['key2int']\n",
    "        int2key_t_minus = dyngraph_info[date_ym_minus]['int2key']\n",
    "        index_t_2_t_minus = [key2int_t_minus[x] if x in key2int_t_minus else -1 for x in int2key_t]\n",
    "        \n",
    "        # Get V_t_plus aligned in V_t index order\n",
    "        V_t_minus_orig = dyngraph_info[date_ym_minus]['vertex_embed']\n",
    "        V_t_minus_orig = np.append(V_t_minus_orig, np.zeros((1, V_t_minus_orig.shape[1])), axis=0)\n",
    "        V_t_minus = V_t_minus_orig[index_t_2_t_minus]\n",
    "                \n",
    "        y_t_minus_orig = dyngraph_info[date_ym_minus]['y']\n",
    "        y_t_minus_orig = np.append(y_t_minus_orig, np.zeros((1,1)), axis=0)\n",
    "        y_t_minus = y_t_minus_orig[index_t_2_t_minus] \n",
    "    else:\n",
    "        V_t_minus = None\n",
    "        y_t_minus = None\n",
    "        key2int_t_minus = None\n",
    "        int2key_t_minus = None\n",
    "    \n",
    "    # Create sparse matrix S\n",
    "    vec_i, vec_j, vec_v = scipy.sparse.find(A_t)\n",
    "    vec_data = vec_v * sigmoid(-np.sum(V_t[vec_i, :] * V_t[vec_j, :], axis=1))\n",
    "    S_t = scipy.sparse.csr_matrix((vec_data, (vec_i, vec_j)), shape=A_t.shape)\n",
    "    \n",
    "    # Create sparse matrix R\n",
    "    l = [[u]*L for u in range(A_t.shape[0])]\n",
    "    smpl_i = [item for sublist in l for item in sublist]\n",
    "    l = [list(np.random.choice(A_t.shape[0], min(L*A_t[u,:].nnz, A_t.shape[0]), replace=False)) for u in range(A_t.shape[0])]\n",
    "    smpl_j = [item for sublist in l for item in sublist]\n",
    "    smpl_index = list(set(zip(smpl_i, smpl_j)) | set(zip(vec_i, vec_j)))\n",
    "    smpl_i, smpl_j = zip(*smpl_index)\n",
    "    smpl_data = sigmoid(np.sum(V_t[smpl_i, :] * V_t[smpl_j, :], axis=1))\n",
    "    R_t = scipy.sparse.csr_matrix((smpl_data, (smpl_i, smpl_j)), shape=A_t.shape)\n",
    "    \n",
    "    # Compute normalization term\n",
    "    norm_graph = 1 / (vec_data.shape[0] + smpl_data.shape[0])\n",
    "    norm_laplacian = 1 / (V_t.shape[0] * V_t.shape[1])\n",
    "    norm_time = 1 / (V_t.shape[0] * V_t.shape[1])\n",
    "    norm_pred = 10 / V_t.shape[0]\n",
    "    \n",
    "    # Update variables\n",
    "    for itr in range(3):\n",
    "        # Create vector z_t\n",
    "        vec_one = np.ones((V_t.shape[0], 1))\n",
    "        tmp_recur = np.cos(theta) * V_t + np.sin(theta) * V_t_minus if V_t_minus is not None else V_t\n",
    "        tmp = np.dot(tmp_recur, w1) + 0.5 * np.dot(tmp_recur * tmp_recur, w2)\n",
    "        pred_t = sigmoid(tmp)\n",
    "        q_t = (vec_one - y_t) * (vec_one - pred_t) + y_t * pred_t + 1e-10\n",
    "        q_t[q_t > 1] = 1.\n",
    "        z_t = focal_alpha * (-focal_gamma * (1 - q_t) ** (focal_gamma - 1) * np.log(q_t) + \\\n",
    "                             1/q_t * (1 - q_t) ** focal_gamma) * \\\n",
    "              (2 * y_t - vec_one) * pred_t * (vec_one - pred_t)\n",
    "        \n",
    "        # Compute the gradient w.r.t. V_t\n",
    "        grad_V_t = norm_graph * (-param_alpha * S_t * V_t + R_t * V_t) + \\\n",
    "                   lambda_V * V_t + \\\n",
    "                   norm_laplacian * L_t * L_t * V_t\n",
    "        \n",
    "        if V_t_plus is not None:\n",
    "            tmp_recur = np.cos(theta) * V_t_plus + np.sin(theta) * V_t\n",
    "            pred_t_plus = sigmoid(np.dot(tmp_recur, w1) + 0.5 * np.dot(tmp_recur**2, w2))\n",
    "            q_t_plus = (vec_one - y_t_plus) * (vec_one - pred_t_plus) + y_t_plus * pred_t_plus + 1e-10\n",
    "            q_t_plus[q_t_plus > 1] = 1.\n",
    "            z_t_plus = focal_alpha * (-focal_gamma * (1 - q_t_plus) ** (focal_gamma - 1) * np.log(q_t_plus) + \\\n",
    "                                      1/q_t_plus * (1 - q_t_plus) ** focal_gamma) * \\\n",
    "                       (2 * y_t_plus - vec_one) * pred_t_plus * (vec_one - pred_t_plus)\n",
    "            grad_V_t += norm_time * -(V_t_plus - V_t) + \\\n",
    "                        norm_pred * -(np.dot(np.cos(theta) * z_t + np.sin(theta) * z_t_plus, w1.T) + \n",
    "                                      np.sin(theta) * (z_t_plus * tmp_recur * w2.T))\n",
    "        \n",
    "        if V_t_minus is not None:\n",
    "            tmp_recur = np.cos(theta) * V_t + np.sin(theta) * V_t_minus\n",
    "            grad_V_t += norm_time * (V_t - V_t_minus) + \\\n",
    "                        norm_pred * -(z_t * tmp_recur * w2.T)\n",
    "        V_t -= lr * grad_V_t\n",
    "        \n",
    "        # Compute the gradient w.r.t. w1 & w2\n",
    "        if V_t_minus is not None:\n",
    "            tmp_recur = np.cos(theta) * V_t + np.sin(theta) * V_t_minus\n",
    "            grad_w1 = -norm_pred * np.dot(tmp_recur.T, z_t) + lambda_w * w1\n",
    "            w1 -= lr * grad_w1\n",
    "            grad_w2 = -norm_pred * 0.5 * np.dot((tmp_recur**2).T, z_t) + lambda_w * w2\n",
    "            w2 -= lr * grad_w2\n",
    "            \n",
    "        # Compute the gradient w.r.t. we\n",
    "        grad_we = np.zeros(we.shape)\n",
    "        for index_attr in range(we.shape[0]):\n",
    "            tmpA_t = A_ts[index_attr]\n",
    "            tmp_is, tmp_js, tmp_vs = scipy.sparse.find(tmpA_t)\n",
    "            tmp = np.mean(np.log(sigmoid(np.sum(V_t[tmp_is,:] * V_t[tmp_js,:], axis=1))) * tmp_vs)\n",
    "            tmp_grad_sigma_we = sigmoid(we[index_attr,0]) * (1 - sigmoid(we[index_attr,0]))\n",
    "            grad_we[index_attr,0] = -param_alpha * tmp * tmp_grad_sigma_we\n",
    "        we -= lr * grad_we\n",
    "            \n",
    "        # Compute the gradient w.r.t. theta\n",
    "        grad_theta = -np.sin(theta) * (np.dot(np.dot(z_t.T, V_t), w1) + \\\n",
    "                                       np.dot(np.dot(z_t.T, tmp_recur * V_t), w2))\n",
    "        if V_t_minus is not None:\n",
    "            grad_theta = np.cos(theta) * (np.dot(np.dot(z_t.T, V_t_minus), w1) + \\\n",
    "                                          np.dot(np.dot(z_t.T, tmp_recur * V_t_minus), w2))\n",
    "        theta -= 0.1 * lr * norm_pred * grad_theta[0][0]\n",
    "        \n",
    "        # Compute the gradient w.r.t. V_t_plus\n",
    "        if V_t_plus is not None:\n",
    "            tmp_recur = np.cos(theta) * V_t_plus + np.sin(theta) * V_t\n",
    "            pred_t_plus = sigmoid(np.dot(tmp_recur, w1) + 0.5 * np.dot(tmp_recur**2, w2))\n",
    "            q_t_plus = (vec_one - y_t_plus) * (vec_one * pred_t_plus) + y_t_plus * pred_t_plus + 1e-10\n",
    "            q_t_plus[q_t_plus > 1] = 1.\n",
    "            z_t_plus = focal_alpha * (-focal_gamma * (1 - q_t_plus) ** (focal_gamma - 1) * np.log(q_t_plus) + \\\n",
    "                                      1/q_t_plus * (1 - q_t_plus) ** focal_gamma) * \\\n",
    "                       (2 * y_t_plus - vec_one) * pred_t_plus * (vec_one - pred_t_plus)\n",
    "            grad_V_t_plus = norm_graph * (-param_alpha * S_t * V_t_plus + R_t * V_t_plus) + \\\n",
    "                            lambda_V * V_t_plus + \\\n",
    "                            norm_laplacian * L_t * L_t * V_t_plus + \\\n",
    "                            norm_time * (V_t_plus - V_t) + \\\n",
    "                            norm_pred * (-z_t_plus * tmp_recur * w2.T)\n",
    "            V_t_plus -= lr * grad_V_t_plus\n",
    "        \n",
    "        # Compute the gradient w.r.t. V_t_minus\n",
    "        if V_t_minus is not None:\n",
    "            grad_V_t_minus = norm_graph * (-param_alpha * S_t * V_t_minus + R_t * V_t_minus) + \\\n",
    "                             lambda_V * V_t_minus + \\\n",
    "                             norm_laplacian * L_t * L_t * V_t_minus + \\\n",
    "                             norm_time * (V_t_minus - V_t)\n",
    "            V_t_minus -= lr * grad_V_t_minus\n",
    "        \n",
    "    return date_ym, V_t, w1, w2, we, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(input_pkg):\n",
    "    t, hetergraph = input_pkg\n",
    "    L_t, A_ts = hetergraph\n",
    "    date_ym = list_date_seq[t]\n",
    "    \n",
    "    key2int_t = dyngraph_info[date_ym]['key2int']\n",
    "    int2key_t = dyngraph_info[date_ym]['int2key']\n",
    "    \n",
    "    y_t = dyngraph_info[date_ym]['y']\n",
    "    V_t = dyngraph_info[date_ym]['vertex_embed']\n",
    "    w1 = classifiers['w1']\n",
    "    w2 = classifiers['w2']\n",
    "    we = classifiers['we']\n",
    "    theta = classifiers['theta']\n",
    "    \n",
    "    A_t = scipy.sparse.csr_matrix(A_ts[0].shape)\n",
    "    for index_attr in range(we.shape[0]):\n",
    "        tmpA_t = A_ts[index_attr]\n",
    "        A_t += sigmoid(we[index_attr,0]) * tmpA_t\n",
    "    \n",
    "    if (t + 1) < seq_length:\n",
    "        date_ym_plus = list_date_seq[t + 1]\n",
    "        \n",
    "        key2int_t_plus = dyngraph_info[date_ym_plus]['key2int']\n",
    "        int2key_t_plus = dyngraph_info[date_ym_plus]['int2key']\n",
    "        index_t_2_t_plus = [key2int_t_plus[x] if x in key2int_t_plus else -1 for x in int2key_t]\n",
    "        \n",
    "        # Get V_t_plus aligned in V_t index order\n",
    "        V_t_plus_orig = dyngraph_info[date_ym_plus]['vertex_embed']\n",
    "        V_t_plus_orig = np.append(V_t_plus_orig, np.zeros((1, V_t_plus_orig.shape[1])), axis=0)\n",
    "        V_t_plus = V_t_plus_orig[index_t_2_t_plus]\n",
    "        \n",
    "        # Get y_t_plus aligned in y_t index order\n",
    "        y_t_plus_orig = dyngraph_info[date_ym_plus]['y']\n",
    "        y_t_plus_orig = np.append(y_t_plus_orig, np.zeros((1,1)), axis=0)\n",
    "        y_t_plus = y_t_plus_orig[index_t_2_t_plus]\n",
    "    else:\n",
    "        V_t_plus = None\n",
    "        y_t_plus = None\n",
    "        key2int_t_plus = None\n",
    "        int2key_t_plus = None\n",
    "\n",
    "    if t > 0:\n",
    "        date_ym_minus = list_date_seq[t - 1]\n",
    "        \n",
    "        key2int_t_minus = dyngraph_info[date_ym_minus]['key2int']\n",
    "        int2key_t_minus = dyngraph_info[date_ym_minus]['int2key']\n",
    "        index_t_2_t_minus = [key2int_t_minus[x] if x in key2int_t_minus else -1 for x in int2key_t]\n",
    "        \n",
    "        # Get V_t_plus aligned in V_t index order\n",
    "        V_t_minus_orig = dyngraph_info[date_ym_minus]['vertex_embed']\n",
    "        V_t_minus_orig = np.append(V_t_minus_orig, np.zeros((1, V_t_minus_orig.shape[1])), axis=0)\n",
    "        V_t_minus = V_t_minus_orig[index_t_2_t_minus]\n",
    "                        \n",
    "        y_t_minus_orig = dyngraph_info[date_ym_minus]['y']\n",
    "        y_t_minus_orig = np.append(y_t_minus_orig, np.zeros((1,1)), axis=0)\n",
    "        y_t_minus = y_t_minus_orig[index_t_2_t_minus] \n",
    "    else:\n",
    "        V_t_minus = None\n",
    "        y_t_minus = None\n",
    "        key2int_t_minus = None\n",
    "        int2key_t_minus = None\n",
    "    \n",
    "    # Create sparse matrix S\n",
    "    vec_i, vec_j, vec_v = scipy.sparse.find(A_t)\n",
    "    vec_data = np.log(sigmoid(np.sum(V_t[vec_i, :] * V_t[vec_j, :], axis=1)))\n",
    "    \n",
    "    # Create sparse matrix R\n",
    "    l = [[u]*L for u in range(A_t.shape[0])]\n",
    "    smpl_i = [item for sublist in l for item in sublist]\n",
    "    l = [list(np.random.choice(A_t.shape[0], min(L*A_t[u,:].nnz, A_t.shape[0]), replace=False)) for u in range(A_t.shape[0])]\n",
    "    smpl_j = [item for sublist in l for item in sublist]\n",
    "    smpl_index = list(set(zip(smpl_i, smpl_j)) | set(zip(vec_i, vec_j)))\n",
    "    smpl_i, smpl_j = zip(*smpl_index)\n",
    "    smpl_data = np.log(sigmoid(-np.sum(V_t[smpl_i, :] * V_t[smpl_j, :], axis=1)))\n",
    "    \n",
    "    # Compute normalization term\n",
    "    norm_graph = 1 / (vec_data.shape[0] + smpl_data.shape[0])\n",
    "    norm_laplacian = 1 / (V_t.shape[0] * V_t.shape[1])\n",
    "    norm_time = 1 / (V_t.shape[0] * V_t.shape[1])\n",
    "    norm_pred = 10 / V_t.shape[0]\n",
    "    \n",
    "    loss_t_graph = norm_graph * (-param_alpha * np.sum(vec_v * vec_data) - np.sum(smpl_data)) + \\\n",
    "                   0.5 * lambda_V * np.sum(V_t ** 2)\n",
    "    \n",
    "    loss_t_laplacian = 0.5 * norm_laplacian * np.sum((L_t * V_t) ** 2)\n",
    "    \n",
    "    loss_t_time = 0\n",
    "    if V_t_plus is not None:\n",
    "        loss_t_time += 0.5 * norm_time * np.sum((V_t - V_t_plus) ** 2)\n",
    "    if V_t_minus is not None:\n",
    "        loss_t_time += 0.5 * norm_time * np.sum((V_t - V_t_minus) ** 2)\n",
    "    \n",
    "    tmp_recur = np.cos(theta) * V_t + np.sin(theta) * V_t_minus if V_t_minus is not None else V_t\n",
    "    pred_t = sigmoid(np.dot(tmp_recur, w1) + 0.5 * np.dot(tmp_recur**2, w2))\n",
    "    vec_one = np.ones((y_t.shape[0], 1))\n",
    "    q_t = (vec_one - y_t) * (vec_one - pred_t) + y_t * pred_t + 1e-10\n",
    "    q_t[q_t > 1] = 1.\n",
    "    loss_t_pred = -norm_pred * np.sum(focal_alpha * (vec_one - q_t)**focal_gamma * np.log(q_t))\n",
    "    \n",
    "    loss_t = loss_t_graph + loss_t_laplacian + loss_t_time + loss_t_pred\n",
    "    \n",
    "    return loss_t, loss_t_graph, loss_t_laplacian, loss_t_time, loss_t_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Variables Initialization\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "for year, month in list_date_seq:\n",
    "    num_vertex_t = len(dyngraph_info[(year, month)]['key2int'])\n",
    "    dyngraph_info[(year, month)]['vertex_embed'] = np.random.rand(num_vertex_t, latent_dim)\n",
    "\n",
    "classifiers = {'w1': np.random.rand(latent_dim, 1),\n",
    "               'w2': np.random.rand(latent_dim, 1),\n",
    "               'we': np.random.rand(len(edge_attrs), 1),\n",
    "               'theta': np.random.rand()}\n",
    "\n",
    "#\n",
    "# Model Training\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "list_input_pkg = []\n",
    "for t in range(len(list_date_seq)):\n",
    "    list_input_pkg.append((t, \n",
    "                           (dyngraph_info[list_date_seq[t]]['graph_laplacian'], \n",
    "                            dyngraph_info[list_date_seq[t]]['graph_adjacent'])\n",
    "                          ))\n",
    "num_cores = 9\n",
    "print(\"Training ......\")\n",
    "for epoch in range(epochs):        \n",
    "    #                              #\n",
    "    # Embedding training           #\n",
    "    # ---------------------------- #\n",
    "    res_list = []\n",
    "    with Pool(processes=num_cores) as p:\n",
    "        max_ = seq_length\n",
    "        with tqdm_notebook(total=max_) as pbar:\n",
    "            for input_pkg, res in tqdm_notebook(enumerate(p.imap_unordered(update_params, list_input_pkg))):\n",
    "                pbar.update()\n",
    "                res_list.append(res)\n",
    "    \n",
    "    date_ym_seq, V_t_seq, w1_seq, w2_seq, we_seq, theta_seq = zip(*res_list)\n",
    "    for index in range(len(date_ym_seq)):\n",
    "        date_ym = date_ym_seq[index]\n",
    "        dyngraph_info[date_ym]['vertex_embed'] = V_t_seq[index]\n",
    "    \n",
    "    classifiers['w1'] = np.mean(w1_seq, axis=0)\n",
    "    classifiers['w2'] = np.mean(w2_seq, axis=0)\n",
    "    tmp_mul = np.exp(np.mean(we_seq, axis=0))\n",
    "    classifiers['we'] = tmp_mul / np.sum(tmp_mul)\n",
    "    classifiers['theta'] = np.mean(theta_seq, axis=0)\n",
    "    \n",
    "    #                              #\n",
    "    # Validation                   #\n",
    "    # ---------------------------- #\n",
    "    res_list = []\n",
    "    with Pool(processes=num_cores) as p:\n",
    "        max_ = seq_length\n",
    "        with tqdm_notebook(total=max_) as pbar:\n",
    "            for input_pkg, res in tqdm_notebook(enumerate(p.imap_unordered(validate, list_input_pkg))):\n",
    "                pbar.update()\n",
    "                res_list.append(res)\n",
    "    \n",
    "    # Save the checkpoint\n",
    "    for t in range(seq_length):\n",
    "        date_ym = list_date_seq[t]\n",
    "        np.save('../data/vertex_embeddings/{}_heter_superv_recur_focal_logisticMF_embed_{}-{}'.format(\n",
    "            weight_type, date_ym[0], date_ym[1]), dyngraph_info[(year, month)]['vertex_embed'])  \n",
    "    \n",
    "    vec_loss = np.mean(res_list, axis=0)\n",
    "    degree = (classifiers['theta'] % (2*np.pi)) / (2*np.pi) * 360\n",
    "    print(\"Epoch {} Loss => total: {:.4f} , g: {:.4f}, l: {:.4f}, t: {:.4f}, p: {:.4f}, theta: {:.3f}, theta_d: {:.3f}\".\n",
    "          format(epoch, vec_loss[0], vec_loss[1], vec_loss[2], vec_loss[3], vec_loss[4], classifiers['theta'], degree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Data Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_by_date(year, month):    \n",
    "    # Fetch graph embedding by customer ID\n",
    "    embed_path = '../data/vertex_embeddings/embed_heter_superv_recur_focal_logisticMF_embed_{}-{}.npy'.format(year, month)\n",
    "    E_t = np.load(embed_path)\n",
    "    print(E_t.shape)\n",
    "    embedding = pd.DataFrame(data=E_t, \n",
    "                             index=range(E_t.shape[0]),\n",
    "                             columns=['embed_{}'.format(i) for i in range(E_t.shape[1])])\n",
    "    with open(\"../data/index_mapping/int2key_{}{}.pickle\".format(str(year), str(month).zfill(2)), 'rb') as f:\n",
    "        int2key_t = pickle.load(f)\n",
    "    embedding['customerno'] = embedding.apply(lambda x: int2key_t[x.name], axis=1)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def get_feature(year, month, pre_fix=\"\", \n",
    "                use_feature_ctbc=True, use_feature_embed=True, use_feature_rankscore=True):\n",
    "    from_date = pd.to_datetime(\"{}/{}/{}\".format(month, 1, year))\n",
    "    to_date = from_date + pd.DateOffset(months=1)\n",
    "    print(\"Construct dataframe in {}.{}\".format(year, month))\n",
    "    \n",
    "    # Get CTBC features\n",
    "    if year == 2017:\n",
    "        file_name = 'aml_all_{}{:02d}_y.csv'.format(str(2018)[2:], 1)\n",
    "    else:\n",
    "        file_name = 'aml_all_{}{:02d}_y.csv'.format(str(year)[2:], month)\n",
    "    pdframe = pd.read_csv('../data/' + file_name)\n",
    "    pdframe = pdframe.set_index('customerno')\n",
    "    \n",
    "    # Attach graph embedding onto the features, weight_type: [weight_none, weight_log, count_larger8000]\n",
    "    embed = get_embedding_by_date(year, month)\n",
    "    pdframe = pdframe.join(embed.set_index('customerno'), on='customerno')\n",
    "    embed_col_names = list(embed.columns)\n",
    "    embed_col_names.remove('customerno')\n",
    "    pdframe.loc[:, embed_col_names] = pdframe.loc[:, embed_col_names].fillna(0)\n",
    "    \n",
    "    return pdframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Get original features from CTBC AML table by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_date_ym = DateYM(2018, 1)\n",
    "to_date_ym = DateYM(2019, 6)\n",
    "list_date_seq = list_date_tuples(from_date_ym, to_date_ym)\n",
    "\n",
    "AML_data = {}\n",
    "embed_col_names = []\n",
    "pdframe_pre = None\n",
    "for date_ym in list_date_seq:\n",
    "    pdframe = get_feature(*date_ym)\n",
    "    AML_data[date_ym] = pd.get_dummies(pdframe, columns=['Customer_Type_Code', 'Customer_Category_Code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_feat = []\n",
    "for x in list(pdframe.columns):\n",
    "    if x.startswith('embed') and not x.endswith('_diff'):\n",
    "        embed_feat.append(x)\n",
    "print('The length of embed_feat = {}'.format(len(embed_feat)))\n",
    "\n",
    "feature_feat = []\n",
    "for x in list(pdframe.columns):\n",
    "    if x.startswith('feature') or x.startswith('SARU'):\n",
    "        feature_feat.append(x)\n",
    "print('The length of feature_feat = {}'.format(len(feature_feat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 MinMaxScalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScalarFrame = pd.DataFrame(index=feature_feat, columns=['max', 'min']).fillna(0)\n",
    "\n",
    "for date in AML_data.keys():\n",
    "    print (date)\n",
    "    tmp_AML_data = AML_data.get(date).describe()\n",
    "    for n in feature_feat:\n",
    "        if tmp_AML_data.loc['max', n] > ScalarFrame.loc[n,'max']:\n",
    "            ScalarFrame.loc[n,'max'] = tmp_AML_data.loc['max', n]\n",
    "        if tmp_AML_data.loc['min', n] < ScalarFrame.loc[n,'min']:\n",
    "            ScalarFrame.loc[n,'min'] = tmp_AML_data.loc['min', n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Construct data by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label    = {}\n",
    "announce = {}\n",
    "FILTER   = {}\n",
    "GRUdict  = {}\n",
    "for n in range(3, len(list_date_seq)):\n",
    "    print (list_date_seq[n])\n",
    "    GRUframe_tmp_1 = AML_data[list_date_seq[n]]    \n",
    "    GRUframe_tmp_1 = GRUframe_tmp_1.drop('Y_SAR', axis=1)\n",
    "    GRUframe_tmp_1[embed_feat] = GRUframe_tmp_1[embed_feat]\n",
    "    GRUframe_tmp_1[feature_feat] = (GRUframe_tmp_1[feature_feat] - ScalarFrame.T[feature_feat].min()) /\\\n",
    "                                   (ScalarFrame.T[feature_feat].max() - ScalarFrame.T[feature_feat].min() + 1e-20)\n",
    "    \n",
    "    GRUframe_tmp_2 = AML_data[list_date_seq[n-1]]\n",
    "    GRUframe_tmp_2 = GRUframe_tmp_2.drop('Y_SAR', axis=1)\n",
    "    GRUframe_tmp_2[embed_feat] = GRUframe_tmp_2[embed_feat]\n",
    "    GRUframe_tmp_2[feature_feat] = (GRUframe_tmp_2[feature_feat] - ScalarFrame.T[feature_feat].min()) /\\\n",
    "                                   (ScalarFrame.T[feature_feat].max() - ScalarFrame.T[feature_feat].min() + 1e-20)\n",
    "    \n",
    "    GRUframe_tmp_3 = AML_data[list_date_seq[n-2]]\n",
    "    GRUframe_tmp_3 = GRUframe_tmp_3.drop('Y_SAR', axis=1)\n",
    "    GRUframe_tmp_3[embed_feat] = GRUframe_tmp_3[embed_feat]\n",
    "    GRUframe_tmp_3[feature_feat] = (GRUframe_tmp_3[feature_feat] - ScalarFrame.T[feature_feat].min()) /\\\n",
    "                                   (ScalarFrame.T[feature_feat].max() - ScalarFrame.T[feature_feat].min() + 1e-20)\n",
    "\n",
    "    GRUframe_tmp_4 = AML_data[list_date_seq[n-3]]\n",
    "    GRUframe_tmp_4 = GRUframe_tmp_4.drop('Y_SAR', axis=1)\n",
    "    GRUframe_tmp_4[embed_feat] = GRUframe_tmp_4[embed_feat]\n",
    "    GRUframe_tmp_4[feature_feat] = (GRUframe_tmp_4[feature_feat] - ScalarFrame.T[feature_feat].min()) /\\\n",
    "                                   (ScalarFrame.T[feature_feat].max() - ScalarFrame.T[feature_feat].min() + 1e-20)\n",
    "\n",
    "    GRUdict[list_date_seq[n]] = np.hstack([GRUframe_tmp_1.values[:,np.newaxis, :], \n",
    "                                           GRUframe_tmp_2.values[:,np.newaxis, :], \n",
    "                                           GRUframe_tmp_3.values[:,np.newaxis, :], \n",
    "                                           GRUframe_tmp_4.values[:,np.newaxis, :]])\n",
    "    label[list_date_seq[n]]    = AML_data[list_date_seq[n]]['Y_SAR'].values\n",
    "    announce[list_date_seq[n]] = AML_data[list_date_seq[n]]['announce'].values\n",
    "    FILTER[list_date_seq[n]]   = AML_data[list_date_seq[n]]['FILTER'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, month in list_date_seq[3:]:\n",
    "    np.savez('../../user_data/CloudMile/data/data_{}_{}.npz'.format(year, month),\n",
    "             GRUdict[(year, month)], \n",
    "             label[(year, month)],\n",
    "             announce[(year, month)], \n",
    "             FILTER[(year, month)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Proposed Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Parameters Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Classifier\n",
    "# ---------------------\n",
    "## focal loss\n",
    "alpha = 1\n",
    "gamma_pos = 6\n",
    "gamma_neg = 2\n",
    "grad_clip = 1\n",
    "lambda_l1 = 0\n",
    "weight_decay = 0   # lambda_l2\n",
    "\n",
    "#\n",
    "# VAT\n",
    "# ---------------------\n",
    "vat_xi = 1e-6\n",
    "vat_eps_pos = 1e2\n",
    "vat_eps_neg = 1e-1\n",
    "vat_ip = 1\n",
    "\n",
    "#\n",
    "# Training process\n",
    "# ---------------------\n",
    "train_batch_size = 128\n",
    "test_batch_size = 32\n",
    "\n",
    "#\n",
    "# Optimizer\n",
    "# ---------------------\n",
    "optim_type = 'rlars'       # ['adam', 'rlars']\n",
    "learn_rate = 1e-4\n",
    "adam_beta1 = 0.9\n",
    "adam_beta2 = 0.999\n",
    "\n",
    "max_epochs = 1600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Model Declaration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, fc_dims, in_dim=256, out_dim=1):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.conv1_dim = math.ceil(self.in_dim)\n",
    "        self.conv2_dim = math.ceil(self.in_dim * 2)\n",
    "        self.conv3_dim = math.ceil(self.in_dim)\n",
    "        self.outdim_en1 = fc_dims[0]\n",
    "        self.outdim_en2 = fc_dims[1]\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.model_conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=in_dim, out_channels=self.conv1_dim, kernel_size=2),\n",
    "            nn.BatchNorm1d(self.conv1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv1_dim, out_channels=self.conv2_dim, kernel_size=2),\n",
    "            nn.BatchNorm1d(self.conv2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels=self.conv2_dim, out_channels=self.conv3_dim, kernel_size=2),\n",
    "            nn.BatchNorm1d(self.conv3_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dropout_1 = 1 - 0.1\n",
    "        dropout_2 = 1 - 0.2\n",
    "        dropout_3 = 1 - 0.2\n",
    "        self.model_fc = nn.Sequential(\n",
    "            # FC 1\n",
    "            nn.Dropout(dropout_1),\n",
    "            nn.Linear(in_features=self.conv3_dim, out_features=self.outdim_en1),\n",
    "            nn.BatchNorm1d(self.outdim_en1),\n",
    "            nn.ReLU(),\n",
    "            # FC 2\n",
    "            nn.Dropout(dropout_2),\n",
    "            nn.Linear(in_features=self.outdim_en1, out_features=self.outdim_en2),\n",
    "            nn.BatchNorm1d(self.outdim_en2),\n",
    "            nn.ReLU(),\n",
    "            # FC 3\n",
    "            nn.Dropout(dropout_3),\n",
    "            nn.Linear(in_features=self.outdim_en2, out_features=self.out_dim),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "        print(\"conv_dim = [{}, {}, {}]\".format(self.conv1_dim, self.conv2_dim, self.conv3_dim))\n",
    "        print(\"fc_dim = [{}/{}, {}/{}, {}/{}, {}]\".format(self.conv3_dim, dropout_1, \n",
    "                                                          self.outdim_en1, dropout_2, \n",
    "                                                          self.outdim_en2, dropout_3, self.out_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model_conv(x)\n",
    "        x = self.model_fc(x.view(x.shape[0], -1))\n",
    "        return x\n",
    "    \n",
    "    def get_trainable_parameters(self):\n",
    "        return (param for param in self.parameters() if param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss2(nn.Module):\n",
    "    def __init__(self, alpha=0.01, gamma_pos=3, gamma_neg=2, logits=False, reduce=True):\n",
    "        super(FocalLoss2, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        gamma_diff = self.gamma_pos - self.gamma_neg\n",
    "        F_loss_pos = self.alpha * targets * (1-pt)**self.gamma_pos * BCE_loss\n",
    "        F_loss_pos = torch.mean(pt)**(-gamma_diff) * F_loss_pos\n",
    "        F_loss_neg = self.alpha * (1 - targets) * (1-pt)**self.gamma_neg * BCE_loss\n",
    "        F_loss = 1 * F_loss_pos + 0.9 * F_loss_neg\n",
    "        \n",
    "        avg_F_loss_pos = torch.sum(F_loss_pos) / torch.sum(targets)\n",
    "        avg_F_loss_neg = torch.sum(F_loss_neg) / torch.sum(1-targets)\n",
    "        \n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss), avg_F_loss_pos, avg_F_loss_neg\n",
    "        else:\n",
    "            return F_loss, F_loss_pos, F_loss_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Virtual Adversarial Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _disable_tracking_bn_stats(model):\n",
    "\n",
    "    def switch_attr(m):\n",
    "        if hasattr(m, 'track_running_stats'):\n",
    "            m.track_running_stats ^= True\n",
    "            \n",
    "    model.apply(switch_attr)\n",
    "    yield\n",
    "    model.apply(switch_attr)\n",
    "\n",
    "    \n",
    "def _l2_normalize(d):\n",
    "    d_reshaped = d.view(d.shape[0], -1, *(1 for _ in range(d.dim() - 2)))\n",
    "    d /= torch.norm(d_reshaped, dim=1, keepdim=True) + 1e-8\n",
    "    return d\n",
    "\n",
    "\n",
    "class VATLoss2(nn.Module):\n",
    "\n",
    "    def __init__(self, xi=1e-6, eps_pos=100, eps_neg=1., ip=1):\n",
    "        \"\"\"VAT loss\n",
    "        :param xi: hyperparameter of VAT (default: 10.0)\n",
    "        :param eps_pos: hyperparameter of VAT (default: 100.0)\n",
    "        :param eps_neg: hyperparameter of VAT (default: 0.1)\n",
    "        :param ip: iteration times of computing adv noise (default: 1)\n",
    "        \"\"\"\n",
    "        super(VATLoss2, self).__init__()\n",
    "        self.xi = xi\n",
    "        self.eps_pos = eps_pos\n",
    "        self.eps_neg = eps_neg\n",
    "        self.ip = ip\n",
    "\n",
    "    def forward(self, model, x, y):\n",
    "        with torch.no_grad():\n",
    "            pred = F.softmax(model(x), dim=1)\n",
    "\n",
    "        # Prepare random unit tensor\n",
    "        d = torch.rand(x.shape).sub(0.5).to(x.device)\n",
    "        d = _l2_normalize(d)\n",
    "\n",
    "        with _disable_tracking_bn_stats(model):\n",
    "            # Calculate adversarial direction\n",
    "            for _ in range(self.ip):\n",
    "                d.requires_grad_()\n",
    "                pred_hat = model(x + self.xi * d)\n",
    "                logp_hat = F.log_softmax(pred_hat, dim=1)\n",
    "                # adv_distance = F.kl_div(logp_hat, pred, reduction='batchmean')  # for PyTorch v1.0\n",
    "                adv_distance = F.kl_div(logp_hat, pred)                           # for PyTorch v0.4\n",
    "                adv_distance.backward()\n",
    "                d = _l2_normalize(d.grad)\n",
    "                model.zero_grad()\n",
    "    \n",
    "            # calc LDS\n",
    "            r_adv = d * (self.eps_pos * y + self.eps_neg * (1-y)).reshape(-1, 1, 1)\n",
    "            pred_hat = model(x + r_adv)\n",
    "            logp_hat = F.log_softmax(pred_hat, dim=1)\n",
    "            # lds = F.kl_div(logp_hat, pred, reduction='batchmean')  # for PyTorch v1.0\n",
    "            lds = F.kl_div(logp_hat, pred)                           # for PyTorch v1.0\n",
    "\n",
    "        return lds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 Training / Testing partition setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_date  = [(2018, 7), (2018, 8), (2018, 9),(2018, 10), (2018, 11), (2018, 12)]\n",
    "testing_date   = [(2019, 1), (2019, 2), (2019, 3), (2019, 4), (2019, 5), (2019, 6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2 Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(map(lambda ym: (ym, \n",
    "                            np.load('../../user_data/CloudMile/data/data_{}_{}.npz'.format(*ym), allow_pickle=True)), \n",
    "                training_date + testing_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(map(lambda ym: data[ym], training_date))\n",
    "test_data = list(map(lambda ym: data[ym], testing_date))\n",
    "\n",
    "X_train_ = np.concatenate([data['arr_0'] for data in train_data])\n",
    "y_train_ = np.concatenate([data['arr_1'] for data in train_data])\n",
    "training_announce = np.concatenate([data['arr_2'] for data in train_data])\n",
    "training_FILTER = np.concatenate([data['arr_3'] for data in train_data])\n",
    "\n",
    "X_test_ = np.concatenate([data['arr_0'] for data in test_data])\n",
    "y_test_ = np.concatenate([data['arr_1'] for data in test_data])\n",
    "testing_announce = np.concatenate([data['arr_2'] for data in test_data])\n",
    "testing_FILTER = np.concatenate([data['arr_3'] for data in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Extracting announced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_[training_announce == 1]\n",
    "y_train = y_train_[training_announce == 1]\n",
    "\n",
    "X_test = X_test_[testing_announce == 1]\n",
    "y_test = y_test_[testing_announce == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.4 Magical rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:,:,0] = 0.1 * np.log10(1e4*X_train[:,:,0]**3 + 1e-10) + 1\n",
    "X_train[:,:,2] = 0.1 * np.log10(1e4*X_train[:,:,2]**5 + 1e-10) + 1\n",
    "X_train[:,:,3] = 0.1 * np.log10(1e4*X_train[:,:,3]**2 + 1e-10) + 1\n",
    "X_train[:,:,6] = 0.1 * np.log10(1e0*X_train[:,:,6]**3 + 1e-10) + 1\n",
    "X_train[:,:,7] = 0.1 * np.log10(1e8*X_train[:,:,7]**5 + 1e-10) + 1\n",
    "X_train[:,:,8] = 0.1 * np.log10(1e8*X_train[:,:,8]**3 + 1e-10) + 1\n",
    "X_train[:,:,9] = 0.1 * np.log10(1e8*X_train[:,:,9]**6 + 1e-10) + 1\n",
    "X_train[:,:,10] = 0.1 * np.log10(1e8*X_train[:,:,10]**2 + 1e-10) + 1\n",
    "X_train[:,:,12] = 0.1 * np.log10(1e9*X_train[:,:,12]**3 + 1e-10) + 1\n",
    "X_train[:,:,13] = 0.1 * np.log10(1e9*X_train[:,:,13]**5 + 1e-10) + 1\n",
    "X_train[:,:,14] = 0.1 * np.log10(1e9*X_train[:,:,14]**3.5 + 1e-10) + 1\n",
    "X_train[:,:,15] = 0.1 * np.log10(1e9*X_train[:,:,15]**6 + 1e-10) + 1\n",
    "X_train[:,:,16] = 0.1 * np.log10(1e15*X_train[:,:,16]**3 + 1e-10) + 1\n",
    "X_train[:,:,18] = 0.1 * np.log10(1e15*X_train[:,:,18]**4 + 1e-10) + 1\n",
    "X_train[:,:,20] = 0.1 * np.log10(1e10*X_train[:,:,20]**8 + 1e-10) + 1\n",
    "X_train[:,:,21] = 0.1 * np.log10(1e8*X_train[:,:,21]**3 + 1e-10) + 1\n",
    "X_train[:,:,22] = 0.1 * np.log10(1e20*X_train[:,:,22]**4 + 1e-10) + 1\n",
    "X_train[:,:,23] = 0.1 * np.log10(1e20*X_train[:,:,23]**4 + 1e-10) + 1\n",
    "X_train[:,:,24] = 0.1 * np.log10(1e10*X_train[:,:,24]**5 + 1e-10) + 1\n",
    "X_train[:,:,26] = 0.1 * np.log10(1e10*X_train[:,:,26]**7 + 1e-10) + 1\n",
    "X_train[:,:,27] = 0.1 * np.log10(1e10*X_train[:,:,27]**3 + 1e-10) + 1\n",
    "X_train[:,:,29] = 0.1 * np.log10(1e10*X_train[:,:,29]**7 + 1e-10) + 1\n",
    "X_train[:,:,30] = 0.1 * np.log10(1e10*X_train[:,:,30]**3 + 1e-10) + 1\n",
    "X_train[:,:,32] = 0.1 * np.log10(1e10*X_train[:,:,32]**3 + 1e-10) + 1\n",
    "X_train[:,:,33] = 0.1 * np.log10(1e10*X_train[:,:,33]**2.5 + 1e-10) + 1\n",
    "X_train[:,:,34] = 0.1 * np.log10(1e10*X_train[:,:,34]**3 + 1e-10) + 1\n",
    "X_train[:,:,35] = 0.1 * np.log10(1e20*X_train[:,:,35]**4 + 1e-10) + 1\n",
    "X_train[:,:,36] = 0.1 * np.log10(1e18*X_train[:,:,36]**5 + 1e-10) + 1\n",
    "X_train[:,:,37] = 0.1 * np.log10(1e20*X_train[:,:,37]**4 + 1e-10) + 1\n",
    "X_train[:,:,38] = 0.1 * np.log10(1e20*X_train[:,:,38]**3 + 1e-10) + 1\n",
    "X_train[:,:,39] = 0.1 * np.log10(1e20*X_train[:,:,39]**3 + 1e-10) + 1\n",
    "X_train[:,:,40] = 0.1 * np.log10(1e20*X_train[:,:,40]**3 + 1e-10) + 1\n",
    "X_train[:,:,41] = 0.1 * np.log10(1e20*X_train[:,:,41]**3 + 1e-10) + 1\n",
    "X_train[:,:,42] = 0.1 * np.log10(1e20*X_train[:,:,42]**3 + 1e-10) + 1\n",
    "X_train[:,:,43] = 0.1 * np.log10(1e20*X_train[:,:,43]**3 + 1e-10) + 1\n",
    "X_train[:,:,44] = 0.1 * np.log10(1e20*X_train[:,:,44]**3 + 1e-10) + 1\n",
    "X_train[:,:,45] = 0.1 * np.log10(1e20*X_train[:,:,45]**3 + 1e-10) + 1\n",
    "X_train[:,:,46] = 0.1 * np.log10(1e20*X_train[:,:,46]**3 + 1e-10) + 1\n",
    "X_train[:,:,47] = 0.1 * np.log10(1e20*X_train[:,:,47]**3 + 1e-10) + 1\n",
    "X_train[:,:,51] = 0.1 * np.log10(1e20*X_train[:,:,51]**3 + 1e-10) + 1\n",
    "X_train[:,:,52] = 0.1 * np.log10(1e20*X_train[:,:,52]**3 + 1e-10) + 1\n",
    "X_train[:,:,53] = 0.1 * np.log10(1e20*X_train[:,:,53]**3 + 1e-10) + 1\n",
    "X_train[:,:,54] = 0.1 * np.log10(1e20*X_train[:,:,54]**3 + 1e-10) + 1\n",
    "X_train[:,:,57] = 0.1 * np.log10(1e20*X_train[:,:,57]**20 + 1e-10) + 1\n",
    "X_train[:,:,58] = 0.1 * np.log10(1e20*X_train[:,:,58]**10 + 1e-10) + 1\n",
    "X_train[:,:,59] = 0.1 * np.log10(1e20*X_train[:,:,59]**8 + 1e-10) + 1\n",
    "X_train[:,:,60] = 0.1 * np.log10(1e20*X_train[:,:,60]**6 + 1e-10) + 1\n",
    "X_train[:,:,61] = 0.1 * np.log10(1e20*X_train[:,:,61]**6 + 1e-10) + 1\n",
    "X_train[:,:,62] = 0.1 * np.log10(1e20*X_train[:,:,62]**5 + 1e-10) + 1\n",
    "X_train[:,:,63] = 0.1 * np.log10(1e20*X_train[:,:,63]**3 + 1e-10) + 1\n",
    "X_train[:,:,64] = 0.1 * np.log10(1e20*X_train[:,:,64]**3 + 1e-10) + 1\n",
    "X_train[:,:,65] = 0.1 * np.log10(1e20*X_train[:,:,65]**3 + 1e-10) + 1\n",
    "X_train[:,:,66] = 0.1 * np.log10(1e20*X_train[:,:,66]**3 + 1e-10) + 1\n",
    "X_train[:,:,67] = 0.1 * np.log10(1e20*X_train[:,:,67]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,68] = 0.1 * np.log10(1e20*X_train[:,:,68]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,69] = 0.1 * np.log10(1e20*X_train[:,:,69]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,70] = 0.1 * np.log10(1e20*X_train[:,:,70]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,71] = 0.1 * np.log10(1e20*X_train[:,:,71]**3 + 1e-10) + 1\n",
    "X_train[:,:,72] = 0.1 * np.log10(1e20*X_train[:,:,72]**3 + 1e-10) + 1\n",
    "X_train[:,:,73] = 0.1 * np.log10(1e20*X_train[:,:,73]**3 + 1e-10) + 1\n",
    "X_train[:,:,74] = 0.1 * np.log10(1e20*X_train[:,:,74]**3 + 1e-10) + 1\n",
    "X_train[:,:,75] = 0.1 * np.log10(1e20*X_train[:,:,75]**3 + 1e-10) + 1\n",
    "X_train[:,:,76] = 0.1 * np.log10(1e20*X_train[:,:,76]**3 + 1e-10) + 1\n",
    "X_train[:,:,77] = 0.1 * np.log10(1e15*X_train[:,:,77]**8 + 1e-10) + 1\n",
    "X_train[:,:,78] = 0.1 * np.log10(1e15*X_train[:,:,78]**8 + 1e-10) + 1\n",
    "X_train[:,:,79] = 0.1 * np.log10(1e20*X_train[:,:,79]**5 + 1e-10) + 1\n",
    "X_train[:,:,80] = 0.1 * np.log10(1e20*X_train[:,:,80]**6 + 1e-10) + 1\n",
    "X_train[:,:,81] = 0.1 * np.log10(1e20*X_train[:,:,81]**8 + 1e-10) + 1\n",
    "X_train[:,:,82] = 0.1 * np.log10(1e20*X_train[:,:,82]**10 + 1e-10) + 1\n",
    "X_train[:,:,83] = 0.1 * np.log10(1e20*X_train[:,:,83]**9.5 + 1e-10) + 1\n",
    "X_train[:,:,84] = 0.1 * np.log10(1e20*X_train[:,:,84]**9.5 + 1e-10) + 1\n",
    "X_train[:,:,85] = 0.1 * np.log10(1e20*X_train[:,:,85]**3 + 1e-10) + 1\n",
    "X_train[:,:,86] = 0.1 * np.log10(1e20*X_train[:,:,86]**13 + 1e-10) + 1\n",
    "X_train[:,:,87] = 0.1 * np.log10(1e20*X_train[:,:,87]**10 + 1e-10) + 1\n",
    "X_train[:,:,88] = 0.1 * np.log10(1e20*X_train[:,:,88]**9 + 1e-10) + 1\n",
    "X_train[:,:,89] = 0.1 * np.log10(1e20*X_train[:,:,89]**8 + 1e-10) + 1\n",
    "X_train[:,:,91] = X_train[:,:,91] ** 5\n",
    "X_train[:,:,93] = X_train[:,:,93] ** 2\n",
    "X_train[:,:,94] = X_train[:,:,94] ** 2\n",
    "X_train[:,:,96] = 0.1 * np.log10(1e20*X_train[:,:,96]**4 + 1e-10) + 1\n",
    "X_train[:,:,97] = 0.1 * np.log10(1e20*X_train[:,:,97]**9 + 1e-10) + 1\n",
    "X_train[:,:,98] = 0.1 * np.log10(1e20*X_train[:,:,98]**8 + 1e-10) + 1\n",
    "X_train[:,:,99] = 0.1 * np.log10(1e20*X_train[:,:,99]**7 + 1e-10) + 1\n",
    "X_train[:,:,100] = 0.1 * np.log10(1e20*X_train[:,:,100]**7 + 1e-10) + 1\n",
    "X_train[:,:,101] = 0.1 * np.log10(1e20*X_train[:,:,101]**5 + 1e-10) + 1\n",
    "X_train[:,:,102] = 0.1 * np.log10(1e20*X_train[:,:,102]**4 + 1e-10) + 1\n",
    "X_train[:,:,103] = 0.1 * np.log10(1e20*X_train[:,:,103]**4 + 1e-10) + 1\n",
    "X_train[:,:,104] = 0.1 * np.log10(1e20*X_train[:,:,104]**10 + 1e-10) + 1\n",
    "X_train[:,:,106] = 0.1 * np.log10(1e20*X_train[:,:,106]**4 + 1e-10) + 1\n",
    "X_train[:,:,107] = 0.1 * np.log10(1e20*X_train[:,:,107]**8 + 1e-10) + 1\n",
    "X_train[:,:,108] = 0.1 * np.log10(1e20*X_train[:,:,108]**7 + 1e-10) + 1\n",
    "X_train[:,:,109] = 0.1 * np.log10(1e20*X_train[:,:,109]**7 + 1e-10) + 1\n",
    "X_train[:,:,110] = 0.1 * np.log10(1e20*X_train[:,:,110]**7 + 1e-10) + 1\n",
    "X_train[:,:,111] = 0.1 * np.log10(1e20*X_train[:,:,111]**6 + 1e-10) + 1\n",
    "X_train[:,:,112] = 0.1 * np.log10(1e20*X_train[:,:,112]**5 + 1e-10) + 1\n",
    "X_train[:,:,113] = 0.1 * np.log10(1e20*X_train[:,:,113]**5 + 1e-10) + 1\n",
    "X_train[:,:,114] = 0.1 * np.log10(1e20*X_train[:,:,114]**8 + 1e-10) + 1\n",
    "X_train[:,:,115] = 0.1 * np.log10(1e20*X_train[:,:,115]**7 + 1e-10) + 1\n",
    "X_train[:,:,116] = 0.1 * np.log10(1e20*X_train[:,:,116]**6 + 1e-10) + 1\n",
    "X_train[:,:,117] = 0.1 * np.log10(1e20*X_train[:,:,117]**6 + 1e-10) + 1\n",
    "X_train[:,:,118] = 0.1 * np.log10(1e20*X_train[:,:,118]**5.5 + 1e-10) + 1\n",
    "X_train[:,:,119] = 0.1 * np.log10(1e20*X_train[:,:,119]**4 + 1e-10) + 1\n",
    "X_train[:,:,124] = 0.1 * np.log10(1e20*X_train[:,:,124]**6 + 1e-10) + 1\n",
    "X_train[:,:,125] = 0.1 * np.log10(1e20*X_train[:,:,125]**7.5 + 1e-10) + 1\n",
    "X_train[:,:,126] = 0.1 * np.log10(1e20*X_train[:,:,126]**11 + 1e-10) + 1\n",
    "X_train[:,:,127] = 0.1 * np.log10(1e20*X_train[:,:,127]**8 + 1e-10) + 1\n",
    "X_train[:,:,128] = 0.1 * np.log10(1e20*X_train[:,:,128]**8 + 1e-10) + 1\n",
    "X_train[:,:,129] = 0.1 * np.log10(1e20*X_train[:,:,129]**8 + 1e-10) + 1\n",
    "X_train[:,:,130] = 0.1 * np.log10(1e20*X_train[:,:,130]**8 + 1e-10) + 1\n",
    "X_train[:,:,132] = 0.1 * np.log10(1e12*X_train[:,:,132]**12 + 1e-10) + 1\n",
    "X_train[:,:,133] = 0.1 * np.log10(1e20*X_train[:,:,133]**5.5 + 1e-10) + 1\n",
    "X_train[:,:,140] = 0.1 * np.log10(1e20*X_train[:,:,140]**4 + 1e-10) + 1\n",
    "X_train[:,:,141] = 0.1 * np.log10(1e20*X_train[:,:,141]**4 + 1e-10) + 1\n",
    "X_train[:,:,142] = 0.1 * np.log10(1e20*X_train[:,:,142]**4 + 1e-10) + 1\n",
    "X_train[:,:,143] = 0.1 * np.log10(1e20*X_train[:,:,143]**4 + 1e-10) + 1\n",
    "X_train[:,:,144] = 0.1 * np.log10(1e20*X_train[:,:,144]**10 + 1e-10) + 1\n",
    "X_train[:,:,146] = 0.1 * np.log10(1e20*X_train[:,:,146]**10 + 1e-10) + 1\n",
    "X_train[:,:,147] = 0.1 * np.log10(1e20*X_train[:,:,147]**9 + 1e-10) + 1\n",
    "X_train[:,:,148] = 0.1 * np.log10(1e20*X_train[:,:,148]**8.5 + 1e-10) + 1\n",
    "X_train[:,:,149] = 0.1 * np.log10(1e20*X_train[:,:,149]**9 + 1e-10) + 1\n",
    "X_train[:,:,150] = 0.1 * np.log10(1e20*X_train[:,:,150]**7.5 + 1e-10) + 1\n",
    "X_train[:,:,151] = 0.1 * np.log10(1e20*X_train[:,:,151]**12 + 1e-10) + 1\n",
    "X_train[:,:,152] = 0.1 * np.log10(1e20*X_train[:,:,152]**20 + 1e-10) + 1\n",
    "X_train[:,:,153] = 0.1 * np.log10(1e20*X_train[:,:,153]**11 + 1e-10) + 1\n",
    "X_train[:,:,154] = 0.1 * np.log10(1e20*X_train[:,:,154]**20 + 1e-10) + 1\n",
    "X_train[:,:,155] = 0.1 * np.log10(1e20*X_train[:,:,155]**16 + 1e-10) + 1\n",
    "X_train[:,:,156] = 0.1 * np.log10(1e20*X_train[:,:,156]**10 + 1e-10) + 1\n",
    "X_train[:,:,157] = 0.1 * np.log10(1e20*X_train[:,:,157]**11 + 1e-10) + 1\n",
    "X_train[:,:,158] = 0.1 * np.log10(1e20*X_train[:,:,158]**9 + 1e-10) + 1\n",
    "X_train[:,:,159] = 0.1 * np.log10(1e20*X_train[:,:,159]**3.5 + 1e-10) + 1\n",
    "X_train[:,:,160] = 0.1 * np.log10(1e20*X_train[:,:,160]**11 + 1e-10) + 1\n",
    "X_train[:,:,166] = 0.1 * np.log10(1e20*X_train[:,:,166]**11 + 1e-10) + 1\n",
    "X_train[:,:,168] = 0.1 * np.log10(1e20*X_train[:,:,168]**14 + 1e-10) + 1\n",
    "X_train[:,:,173] = 0.1 * np.log10(1e30*X_train[:,:,173]**7 + 1e-10) + 1\n",
    "X_train[:,:,174] = 0.1 * np.log10(1e30*X_train[:,:,174]**4 + 1e-10) + 1\n",
    "X_train[:,:,178] = 0.1 * np.log10(1e30*X_train[:,:,178]**4 + 1e-10) + 1\n",
    "X_train[:,:,179] = 0.1 * np.log10(1e30*X_train[:,:,179]**4 + 1e-10) + 1\n",
    "X_train[:,:,181] = 0.1 * np.log10(1e30*X_train[:,:,181]**7 + 1e-10) + 1\n",
    "X_train[:,:,182] = 0.1 * np.log10(1e30*X_train[:,:,182]**2 + 1e-10) + 1\n",
    "X_train[:,:,183] = 0.1 * np.log10(1e30*X_train[:,:,183]**2 + 1e-10) + 1\n",
    "X_train[:,:,185] = 0.1 * np.log10(1e30*X_train[:,:,185]**2 + 1e-10) + 1\n",
    "X_train[:,:,186] = 0.1 * np.log10(1e30*X_train[:,:,186]**2 + 1e-10) + 1\n",
    "X_train[:,:,187] = 0.1 * np.log10(1e30*X_train[:,:,187]**2 + 1e-10) + 1\n",
    "X_train[:,:,190] = 0.1 * np.log10(1e10*X_train[:,:,190]**4 + 1e-10) + 1\n",
    "X_train[:,:,191] = 0.1 * np.log10(1e20*X_train[:,:,191]**6 + 1e-10) + 1\n",
    "X_train[:,:,192] = 0.1 * np.log10(1e20*X_train[:,:,192]**4 + 1e-10) + 1\n",
    "X_train[:,:,195] = 0.1 * np.log10(1e20*X_train[:,:,195]**4 + 1e-10) + 1\n",
    "X_train[:,:,196] = 0.1 * np.log10(1e20*X_train[:,:,196]**6 + 1e-10) + 1\n",
    "X_train[:,:,197] = 0.1 * np.log10(1e20*X_train[:,:,197]**4 + 1e-10) + 1\n",
    "X_train[:,:,198] = 0.1 * np.log10(1e20*X_train[:,:,198]**6 + 1e-10) + 1\n",
    "X_train[:,:,200] = 0.1 * np.log10(1e20*X_train[:,:,200]**6 + 1e-10) + 1\n",
    "X_train[:,:,201] = 0.1 * np.log10(1e20*X_train[:,:,201]**5 + 1e-10) + 1\n",
    "X_train[:,:,202] = 0.1 * np.log10(1e20*X_train[:,:,202]**5 + 1e-10) + 1\n",
    "X_train[:,:,203] = 0.1 * np.log10(1e20*X_train[:,:,203]**5 + 1e-10) + 1\n",
    "X_train[:,:,204] = 0.1 * np.log10(1e30*X_train[:,:,204]**3 + 1e-10) + 1\n",
    "X_train[:,:,205] = 0.1 * np.log10(1e30*X_train[:,:,205]**3 + 1e-10) + 1\n",
    "X_train[:,:,206] = 0.1 * np.log10(1e20*X_train[:,:,206]**7 + 1e-10) + 1\n",
    "X_train[:,:,207] = 0.1 * np.log10(1e30*X_train[:,:,207]**2 + 1e-10) + 1\n",
    "X_train[:,:,208] = 0.1 * np.log10(1e30*X_train[:,:,208]**2 + 1e-10) + 1\n",
    "X_train[:,:,209] = 0.1 * np.log10(1e30*X_train[:,:,209]**2 + 1e-10) + 1\n",
    "X_train[:,:,213] = 0.1 * np.log10(1e30*X_train[:,:,213]**2 + 1e-10) + 1\n",
    "X_train[:,:,214] = 0.1 * np.log10(1e7*X_train[:,:,214]**2 + 1e-10) + 1\n",
    "X_train[:,:,215] = 0.1 * np.log10(1e15*X_train[:,:,215]**2 + 1e-10) + 1\n",
    "X_train[:,:,216] = 0.1 * np.log10(1e15*X_train[:,:,216]**2 + 1e-10) + 1\n",
    "X_train[:,:,217] = 0.1 * np.log10(1e15*X_train[:,:,217]**2 + 1e-10) + 1\n",
    "X_train[:,:,218] = 0.1 * np.log10(1e15*X_train[:,:,218]**2 + 1e-10) + 1\n",
    "X_train[:,:,220] = 0.1 * np.log10(1e15*X_train[:,:,220]**2 + 1e-10) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.5 Converting Numpy tensor into PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(X_train)):\n",
    "    train_data.append((X_train[i], y_train[i]))\n",
    "    \n",
    "test_data = []\n",
    "for i in range(len(X_test)):\n",
    "    test_data.append((X_test[i], y_test[i]))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Model Initialization\n",
    "\n",
    "### 5.4.1 Classifier / Focal Loss initialization\n",
    "Notice that the dimensions per layer are recommended to set to a smaller value due to the risk of overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ConvNet(fc_dims=[128, 64], in_dim=X_train.shape[2], out_dim=2).cuda()\n",
    "\n",
    "focal_loss = FocalLoss2(alpha, gamma_pos, gamma_neg)\n",
    "if optim_type == \"adam\":\n",
    "    optim_clsfr = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), \n",
    "                             lr=learn_rate)\n",
    "else:\n",
    "    optim_clsfr = RangerLars(filter(lambda p: p.requires_grad, classifier.parameters()),\n",
    "                             lr=learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.2 VAT Loss initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vat_loss2 = VATLoss2(xi=vat_xi, eps_pos=vat_eps_pos, eps_neg=vat_eps_neg, ip=vat_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Training / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, dataloader, clip_grad_norm=0, lambda_l1=1e-3):\n",
    "    label_list = []\n",
    "    pred_y_list = []\n",
    "    \n",
    "    clsf_loss_batch = []\n",
    "    clsf_loss_pos_batch = []\n",
    "    clsf_loss_neg_batch = []\n",
    "    vat_batch = []\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        if data.size()[0] != dataloader.batch_size:\n",
    "            continue\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        tmp = target.reshape(-1, 1)\n",
    "        onehot_target = torch.cat([1-tmp, tmp], dim=1)\n",
    "                \n",
    "        #\n",
    "        # Update classifier on real samples\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        optim_clsfr.zero_grad()\n",
    "        \n",
    "        # VAT Loss\n",
    "        vat_kld = vat_loss2(classifier, data.permute(0, 2, 1), target)     # for Conv1D\n",
    "        \n",
    "        # Focal Loss\n",
    "        pred_y = classifier(data.permute(0, 2, 1)).squeeze(-1)\n",
    "        clsf_loss, clsf_loss_pos, clsf_loss_neg = focal_loss(pred_y, onehot_target)\n",
    "        \n",
    "        # L1 Regularization Loss\n",
    "        regularizer_loss_l1 = 0\n",
    "        for param in classifier.parameters():\n",
    "            regularizer_loss_l1 += torch.sum(torch.abs(param))\n",
    "        \n",
    "        # Total Loss\n",
    "        loss = clsf_loss + 10 * vat_kld + lambda_l1 * regularizer_loss_l1\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if clip_grad_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(classifier.parameters(), max_norm=clip_grad_norm)\n",
    "        \n",
    "        optim_clsfr.step()\n",
    "        \n",
    "        #\n",
    "        # Record the losses\n",
    "        # ------------------------------------------------------------------------------------\n",
    "        pred_yy = torch.nn.functional.softmax(pred_y, dim=1)[:, 1]\n",
    "        vat_batch.append(vat_kld)\n",
    "        clsf_loss_batch.append(clsf_loss)\n",
    "        if torch.sum(target) > 0:\n",
    "            clsf_loss_pos_batch.append(clsf_loss_pos)\n",
    "        clsf_loss_neg_batch.append(clsf_loss_neg)\n",
    "        \n",
    "        label_list += list(target.cpu().detach().numpy())\n",
    "        pred_y_list += list(pred_yy.cpu().detach().numpy())\n",
    "    \n",
    "    vat_loss_avg = sum(vat_batch) / len(vat_batch)\n",
    "    clsf_loss_avg = sum(clsf_loss_batch) / len(clsf_loss_batch)\n",
    "    clsf_loss_pos_avg = sum(clsf_loss_pos_batch) / len(clsf_loss_pos_batch)\n",
    "    clsf_loss_neg_avg = sum(clsf_loss_neg_batch) / len(clsf_loss_neg_batch)\n",
    "    \n",
    "    return np.array(label_list), np.array(pred_y_list), clsf_loss_avg, clsf_loss_pos_avg, clsf_loss_neg_avg, vat_loss_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(dataloader):\n",
    "    label_list = []\n",
    "    pred_y_list = []   \n",
    "    \n",
    "    clsf_loss_batch = []\n",
    "    clsf_loss_pos_batch = []\n",
    "    clsf_loss_neg_batch = []\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        if data.size()[0] != dataloader.batch_size:\n",
    "            continue\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        tmp = target.reshape(-1, 1)\n",
    "        onehot_target = torch.cat([1-tmp, tmp], dim=1)\n",
    "        \n",
    "        # Update classifier\n",
    "        pred_y = classifier(data.permute(0, 2, 1)).squeeze(-1)     # for Conv1D\n",
    "        # pred_y_softmax = torch.nn.functional.softmax(pred_y, dim=1)[:, 1]\n",
    "        \n",
    "        clsf_loss, clsf_loss_pos, clsf_loss_neg = focal_loss(pred_y, onehot_target)\n",
    "        clsf_loss_batch.append(clsf_loss)\n",
    "        if torch.sum(target) > 0:\n",
    "            clsf_loss_pos_batch.append(clsf_loss_pos)\n",
    "        clsf_loss_neg_batch.append(clsf_loss_neg)\n",
    "        \n",
    "        label_list += list(target.cpu().detach().numpy())\n",
    "        pred_y_list += list(pred_y[:, 1].cpu().detach().numpy())\n",
    "    \n",
    "    clsf_loss_avg = sum(clsf_loss_batch) / len(clsf_loss_batch)\n",
    "    clsf_loss_pos_avg = sum(clsf_loss_pos_batch) / len(clsf_loss_pos_batch)\n",
    "    clsf_loss_neg_avg = sum(clsf_loss_neg_batch) / len(clsf_loss_neg_batch)\n",
    "    \n",
    "    return np.array(label_list), np.array(pred_y_list), clsf_loss_avg, clsf_loss_pos_avg, clsf_loss_neg_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    prec = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return prec, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameter Setting ----------------------------------------------------------------------')\n",
    "print('Model = VAT + Conv1D')\n",
    "print('conv1d use activation = {}'.format(True))\n",
    "print('graph_emdeding = Heter_AutoEncoder')\n",
    "print('alpha = {}'.format(alpha))\n",
    "print('gamma_pos = {}'.format(gamma_pos))\n",
    "print('gamma_neg = {}'.format(gamma_neg))\n",
    "print('learn_rate = {}'.format(learn_rate))\n",
    "print('train_batch_size = {}'.format(train_batch_size))\n",
    "print('test_batch_size = {}'.format(test_batch_size))\n",
    "print('max_epochs = {}'.format(max_epochs))\n",
    "print('vat_xi = {}'.format(vat_xi))\n",
    "print('vat_eps_pos = {}'.format(vat_eps_pos))\n",
    "print('vat_eps_neg = {}'.format(vat_eps_neg))\n",
    "print('vat_ip = {}'.format(vat_ip))\n",
    "print('optim_type = {}'.format(optim_type))\n",
    "print('weight_decay = {}'.format(weight_decay))\n",
    "print('lambda_l1 = {}'.format(lambda_l1))\n",
    "print('\\n')\n",
    "\n",
    "train_history_loss = []\n",
    "train_history_auc = []\n",
    "max_f1score = 0\n",
    "for epoch in range(max_epochs):\n",
    "    print('Epoch {} -------------------------------------------------------------------------'.format(epoch))\n",
    "    \n",
    "    #\n",
    "    # Training\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    classifier.train()\n",
    "    label_train, pred_y_train, clsf_loss_train, clsf_loss_pos_train, \\\n",
    "        clsf_loss_neg_train, vat_loss_train = train(epoch, train_dataloader, \n",
    "                                                    clip_grad_norm=grad_clip, lambda_l1=lambda_l1)\n",
    "    \n",
    "    auc_train = roc_auc_score(label_train, pred_y_train)\n",
    "    train_history_loss.append(clsf_loss_train)\n",
    "    train_history_auc.append(auc_train)\n",
    "    print('    Training => auc: {:.6f}, clsf_pos: {}, clsf_neg: {}, vat_loss: {}'.\n",
    "          format(auc_train, clsf_loss_pos_train, clsf_loss_neg_train, vat_loss_train))\n",
    "    thres = np.min(pred_y_train[label_train==1]) - 1e-6\n",
    "    print(\"                Threshold is set to {}\".format(thres))\n",
    "    print(\"                Min. Probailities on train is {}\".format(np.min(pred_y_train)))\n",
    "    \n",
    "    y_predict_bin = np.array(pred_y_train > thres, dtype=int)\n",
    "    prec_train, recall_train, f1_train = evaluate(label_train, y_predict_bin)\n",
    "    print('                prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}'.\n",
    "          format(prec_train, recall_train, f1_train))\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        #\n",
    "        # Testing\n",
    "        # ------------------------------------------------------------------------------------        \n",
    "        with torch.no_grad():\n",
    "            classifier.eval()\n",
    "            label_test, pred_y_test, clsf_loss_test, clsf_loss_pos_test, clsf_loss_neg_test = infer(test_dataloader)    \n",
    "        \n",
    "        auc = roc_auc_score(label_test, pred_y_test)\n",
    "        \n",
    "        print(\"            Min. Probailities on test set with label 1: {}\".format(np.min(pred_y_test[label_test==1])))\n",
    "        print(\"            Min. Probailities on test set             : {}\".format(np.min(pred_y_test)))\n",
    "        \n",
    "        y_predict_bin = np.array(pred_y_test > thres, dtype=int)\n",
    "        prec, recall_bytrain, f1_bytrain = evaluate(label_test, y_predict_bin)\n",
    "        print('    Testing by train ==> auc: {:.6f}, prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}, clsf_loss: {}'.\n",
    "              format(auc, prec, recall_bytrain, f1_bytrain, clsf_loss_test))\n",
    "        \n",
    "        y_predict_bin = np.array(pred_y_test >= np.min(pred_y_test[label_test==1]), dtype=int)\n",
    "        prec, recall_bytest, f1_bytest = evaluate(label_test, y_predict_bin)\n",
    "        print('    Testing by test ==> auc: {:.6f}, prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}, clsf_loss: {}'.\n",
    "              format(auc, prec, recall_bytest, f1_bytest, clsf_loss_test))\n",
    "        \n",
    "        if clsf_loss_pos_train < 0.005 and f1_bytrain > max_f1score and recall_bytrain == 1:\n",
    "            max_f1score = f1_bytrain\n",
    "            torch.save({'epoch': epoch,\n",
    "                        'model_state_dict': classifier.state_dict(),\n",
    "                        'optimizer_state_dict': optim_clsfr.state_dict(),\n",
    "                        'loss': focal_loss,\n",
    "                       }, \n",
    "                       '../../user_data/CloudMile/data/saved_models/' + \\\n",
    "                       'FeatEmbed-Conv1Dsmall_v2_eps{}{}_focal{}{}_{}_lr{}_epoch{}_Part2_{}'.\n",
    "                       format(int(-math.log10(vat_eps_pos)), \n",
    "                              int(-math.log10(vat_eps_neg)), \n",
    "                              gamma_pos, gamma_neg, \n",
    "                              optim_type, int(-math.log10(learn_rate)), max_epochs, f1_bytrain))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Inference Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(pred_y_list, label_list, save_path=False, stat='Train'):\n",
    "    ###########################################################\n",
    "    plt.ylabel('Log Count')\n",
    "    plt.xlabel('Score')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['left'].set_color('none')\n",
    "    ax.spines['bottom'].set_color('none')\n",
    "    \n",
    "    MIN = 0.4\n",
    "    MAX = 0.5\n",
    "    BIN = 100\n",
    "    plt.grid(color = '#9999CC')\n",
    "    plt.hist(pred_y_list[np.where(label_list == 0)], \n",
    "             bins=[n/(10*BIN) for n in range(int(10*MIN)*BIN, int(10*MAX)*BIN)], \n",
    "             label='Negative',\n",
    "             color='#598987')\n",
    "    plt.hist(pred_y_list[np.where(label_list == 1)], \n",
    "             bins=[n/(10*BIN) for n in range(int(10*MIN)*BIN, int(10*MAX)*BIN)], \n",
    "             label='Positive', \n",
    "             color='#FFD000')\n",
    "    plt.legend(loc='upper right')\n",
    "    if save_path:\n",
    "        plt.savefig(\"result/{}_{}.jpg\".format(save_path, stat), dpi=1000, quality=100)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(PATH, save_path=False):\n",
    "    model = ConvNet(fc_dims=[128, 64], in_dim=X_train.shape[2], out_dim=2).cuda()\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    ##### Training #####\n",
    "    Training_label_list = []\n",
    "    Training_pred_y_list = []\n",
    "\n",
    "    for batch_idx, (data, target) in tqdm_notebook(enumerate(train_dataloader)):\n",
    "        if data.size()[0] != train_dataloader.batch_size:\n",
    "            continue\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())\n",
    "        Training_pred_y = model(data.permute(0, 2, 1)).squeeze(-1)     # for Conv1D\n",
    "        Training_pred_y = torch.nn.functional.softmax(Training_pred_y, dim=1)[:, 1]\n",
    "\n",
    "        Training_label_list += list(target.cpu().detach().numpy())\n",
    "        Training_pred_y_list += list(Training_pred_y.cpu().detach().numpy())\n",
    "    \n",
    "    \n",
    "    Training_label_list = np.array(Training_label_list)\n",
    "    Training_pred_y_list = np.array(Training_pred_y_list)\n",
    "    auc_train = roc_auc_score(Training_label_list, Training_pred_y_list)\n",
    "    \n",
    "    ##### Testing #####\n",
    "    Testing_label_list = []\n",
    "    Testing_pred_y_list = []\n",
    "    for batch_idx, (data, target) in tqdm_notebook(enumerate(test_dataloader)):\n",
    "        if data.size()[0] != test_dataloader.batch_size:\n",
    "            continue\n",
    "        data, target = Variable(data.cuda()), Variable(target.cuda())        \n",
    "        Testing_pred_y = model(data.permute(0, 2, 1)).squeeze(-1)     # for Conv1D\n",
    "        Testing_pred_y = torch.nn.functional.softmax(Testing_pred_y, dim=1)[:, 1]\n",
    "\n",
    "        Testing_label_list += list(target.cpu().detach().numpy())\n",
    "        Testing_pred_y_list += list(Testing_pred_y.cpu().detach().numpy())\n",
    "    \n",
    "    Testing_label_list = np.array(Testing_label_list)\n",
    "    Testing_pred_y_list = np.array(Testing_pred_y_list)\n",
    "    auc_test = roc_auc_score(Testing_label_list, Testing_pred_y_list)\n",
    "    \n",
    "    ### Performance \n",
    "    ratio = .05\n",
    "    thres1 = np.min(Training_pred_y_list[np.where(Training_label_list == 1)])\n",
    "    thres2 = sorted(Testing_pred_y_list, reverse=True)[int(Testing_label_list.sum() + int(len(Testing_label_list)*ratio))]\n",
    "    \n",
    "    print ('Cut1: {}'.format(thres1))\n",
    "    print ('Cut2: {}'.format(thres2))\n",
    "\n",
    "    print(\"Recall of Training = 1 : {}\".format(thres1 > np.min(Training_pred_y_list)))\n",
    "    print('Training => auc: {:.6f}'.format(auc_train))\n",
    "    print (\"Training Treshold: {}\".format(thres1))\n",
    "    \n",
    "    y_predict_bin = np.array(Training_pred_y_list >= thres1, dtype=int)\n",
    "    prec_train, recall_train, f1_train = evaluate(Training_label_list, y_predict_bin)\n",
    "    print('prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}'.format(prec_train, recall_train, f1_train))\n",
    "    print (\"Total Positve: {}\".format(int(sum(Training_label_list))))\n",
    "    num_cand = np.sum(Training_pred_y_list >= thres1)\n",
    "    print (\"Total Candidate: {}\".format(num_cand))\n",
    "    print('----------------------------------------------')\n",
    "    \n",
    "    ## High Risk\n",
    "    print(\"High Risk Positve: {}\".format(np.sum(Training_label_list[Training_pred_y_list >= thres1])))\n",
    "    print(\"High Risk Candidate: {}\".format(np.sum(Training_pred_y_list >= thres1)))\n",
    "    print(\"High Risk Prec: {}\".format(np.sum(Training_label_list[np.where(Training_pred_y_list >= thres1)])/\\\n",
    "                                      np.sum(Training_pred_y_list >= thres1)))\n",
    "    print('----------------------------------------------')\n",
    "    ## Medium Risk\n",
    "    print(\"Medium Risk Positve: {}\".format(np.sum(Training_label_list[(Training_pred_y_list < thres1) &\\\n",
    "                                                                     (Training_pred_y_list >= thres2)])))\n",
    "    print(\"Medium Risk Candidate: {}\".format(np.sum((Training_pred_y_list < thres1) &\\\n",
    "                                                    (Training_pred_y_list >= thres2))))\n",
    "    print(\"Medium Risk Prec: {}\".format(np.sum(Training_label_list[np.where((Training_pred_y_list < thres1) &\\\n",
    "                                                                           (Training_pred_y_list >= thres2))])/\\\n",
    "                                                             np.sum((Training_pred_y_list < thres1) &\\\n",
    "                                                                    (Training_pred_y_list >= thres2))))\n",
    "    \n",
    "    plotting(Training_pred_y_list, Training_label_list, save_path)\n",
    "#     pd.DataFrame([Training_pred_y_list, Training_label_list, training_announce.tolist()], \n",
    "#                  index=['score', 'label', 'annouce']).T.to_csv('result/Training_result.csv', index=None)\n",
    "    ####　----------------------------------------------\n",
    "    print(\"Recall of Testing = 1 : {}\".format(thres2 <= np.min(Testing_pred_y_list[Testing_label_list == 1])))\n",
    "    print('Testing => auc: {:.6f}'.format(auc_test))\n",
    "    print(\"Threshold is set to {}\".format(thres2))\n",
    "    \n",
    "    y_predict_bin = np.array(Testing_pred_y_list >= thres2, dtype=int)\n",
    "    prec_test, recall_test, f1_test = evaluate(Testing_label_list, y_predict_bin)\n",
    "    print('prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}'.format(prec_test, recall_test, f1_test))\n",
    "    print (\"Total Positve: {}\".format(int(sum(Testing_label_list))))\n",
    "    print(\"Total Candidate: {}\".format(np.sum(Testing_pred_y_list >= thres2)))\n",
    "    print('----------------------------------------------')\n",
    "    \n",
    "    ## High Risk\n",
    "    print(\"High Risk Positve: {}\".format(np.sum(Testing_label_list[Testing_pred_y_list >= thres1])))\n",
    "    print(\"High Risk Candidate: {}\".format(np.sum(Testing_pred_y_list >= thres1)))\n",
    "    print(\"High Risk Prec: {}\".format(np.sum(Testing_label_list[np.where(Testing_pred_y_list >= thres1)])/\\\n",
    "                                      np.sum(Testing_pred_y_list >= thres1)))\n",
    "    print('----------------------------------------------')\n",
    "    ## Medium Risk\n",
    "    print(\"Medium Risk Positve: {}\".format(np.sum(Testing_label_list[(Testing_pred_y_list < thres1) &\\\n",
    "                                                                     (Testing_pred_y_list >= thres2)])))\n",
    "    print(\"Medium Risk Candidate: {}\".format(np.sum((Testing_pred_y_list < thres1) &\\\n",
    "                                                    (Testing_pred_y_list >= thres2))))\n",
    "    print(\"Medium Risk Prec: {}\".format(np.sum(Testing_label_list[np.where((Testing_pred_y_list < thres1) &\\\n",
    "                                                                           (Testing_pred_y_list >= thres2))])/\\\n",
    "                                                             np.sum((Testing_pred_y_list < thres1) &\\\n",
    "                                                                    (Testing_pred_y_list >= thres2))))\n",
    "\n",
    "    \n",
    "    plotting(Testing_pred_y_list, Testing_label_list, save_path, stat='Test')\n",
    "#     pd.DataFrame([Testing_pred_y_list, Testing_label_list, testing_announce.tolist()], \n",
    "#                  index=['score', 'label', 'annouce']).T.to_csv('result/Testing_result.csv', index=None)\n",
    "#     return Training_label_list, Training_pred_y_list, Testing_label_list, Testing_pred_y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '{model path}'\n",
    "performance(PATH, 'FeatEmbed-Golden-LastSoftmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Other Model\n",
    "##  7.1 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= [np.load('../../user_data/CloudMile/data/data_{}_{}.npz'.format(year, month), \n",
    "                     allow_pickle=True) \n",
    "             for year, month in training_date]\n",
    "\n",
    "test_data= [np.load('../../user_data/CloudMile/data/data_{}_{}.npz'.format(year,  month), \n",
    "                    allow_pickle=True) \n",
    "            for year, month in testing_date]\n",
    "\n",
    "X_train = np.concatenate([data['arr_0'] for data in train_data])\n",
    "y_train = np.concatenate([data['arr_1'] for data in train_data])\n",
    "training_announce = np.concatenate([data['arr_2'] for data in train_data])\n",
    "training_FILTER = np.concatenate([data['arr_3'] for data in train_data])\n",
    "\n",
    "X_test = np.concatenate([data['arr_0'] for data in test_data])\n",
    "y_test = np.concatenate([data['arr_1'] for data in test_data])\n",
    "testing_announce = np.concatenate([data['arr_2'] for data in test_data])\n",
    "testing_FILTER = np.concatenate([data['arr_3'] for data in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:,0,:]\n",
    "X_test = X_test[:,0,:]\n",
    "\n",
    "## Get Feat\n",
    "X_train = np.hstack([X_train[:,:314], X_train[:,-14:]])\n",
    "X_test = np.hstack([X_test[:,:314], X_test[:,-14:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Announce Extract\n",
    "X_train = X_train[training_announce == 1]\n",
    "y_train = y_train[training_announce == 1]\n",
    "\n",
    "X_test = X_test[testing_announce == 1]\n",
    "y_test = y_test[testing_announce == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model initial & training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(n_estimators=100,\n",
    "                    learning_rate= 0.1,     \n",
    "                    max_depth=5, \n",
    "                    subsample=1,\n",
    "                    gamma=0, \n",
    "                    reg_lambda=1, \n",
    "                    max_delta_step=0,\n",
    "                    colsample_bytree=1, \n",
    "                    min_child_weight=1, \n",
    "                    seed=1000)\n",
    "\n",
    "clf.fit(X=X_train, y=y_train, eval_set=[(X_test, y_test)] , eval_metric='auc', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_train = clf.predict_proba(X_train)[:,1]\n",
    "pred_y_test  = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(pred_y_list, label_list, save_path=False, stat='Train'):\n",
    "    ###########################################################\n",
    "    plt.ylabel('Log Count')\n",
    "    plt.xlabel('Score')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['left'].set_color('none')\n",
    "    ax.spines['bottom'].set_color('none')\n",
    "    \n",
    "    plt.grid(color = '#9999CC')\n",
    "    plt.hist(pred_y_list[np.where(label_list == 0)], \n",
    "             bins=[n/200 for n in range(200)], \n",
    "             label='Negative',\n",
    "             color='#598987')\n",
    "    plt.hist(pred_y_list[np.where(label_list == 1)], \n",
    "             bins=[n/200 for n in range(200)], \n",
    "             label='Positive', \n",
    "             color='#FFD000')\n",
    "    plt.legend(loc='upper right')\n",
    "    if save_path:\n",
    "        plt.savefig(\"result/{}_{}_1.jpg\".format(save_path, stat), dpi=1000, quality=100)\n",
    "    plt.show()\n",
    "    ###########################################################\n",
    "    plt.ylabel('Log Count')\n",
    "    plt.xlabel('Score')\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['left'].set_color('none')\n",
    "    ax.spines['bottom'].set_color('none')\n",
    "    \n",
    "    plt.grid(color = '#9999CC')\n",
    "    plt.hist(pred_y_list[np.where(label_list == 0)], \n",
    "             bins=[n/1000 for n in range(35)], \n",
    "             label='Negative', \n",
    "             color='#598987')\n",
    "    plt.hist(pred_y_list[np.where(label_list == 1)], \n",
    "             bins=[n/1000 for n in range(35)], \n",
    "             label='Positive', \n",
    "             color='#FFD000')\n",
    "    plt.legend(loc='upper right')\n",
    "    if save_path:\n",
    "        plt.savefig(\"result/{}_{}_2.jpg\".format(save_path, stat), dpi=1000, quality=100)\n",
    "    plt.show()\n",
    "\n",
    "def performance(y_train, pred_y_train, y_test, pred_y_test, save_path=False):   \n",
    "    auc_train = roc_auc_score(y_train, pred_y_train)\n",
    "    auc_test  = roc_auc_score(y_test, pred_y_test)\n",
    "    \n",
    "    thres1 = np.min(pred_y_train[np.where(y_train == 1)])\n",
    "    print('Training => auc: {:.6f}'.format(auc_train))\n",
    "    print(\"Threshold is set to {}\".format(thres1))\n",
    "    \n",
    "    y_predict_bin = np.array(pred_y_train >= thres1, dtype=int)\n",
    "    prec_train, recall_train, f1_train = evaluate(y_train, y_predict_bin)\n",
    "    print('prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}'.format(prec_train, recall_train, f1_train))\n",
    "    \n",
    "    print (\"Total Positve: {}\".format(len(np.where(y_train == 1)[0])))\n",
    "    num_cand = np.sum(pred_y_train >= thres1)\n",
    "    print (\"Total Candidate: {}\".format(num_cand))\n",
    "    plotting(pred_y_train, y_train, save_path)\n",
    "    \n",
    "#     pd.DataFrame([pred_y_train, y_train, training_announce.tolist()], \n",
    "#                  index=['score', 'label', 'annouce']).T.to_csv('result/Feat_XGB_Training_res_Part2.csv', index=None)\n",
    "    \n",
    "    ##### Testing #####\n",
    "    ratio = 0.05\n",
    "    thres2 = sorted(pred_y_test, reverse=True)[int(y_test.sum() + int(len(y_test)*ratio))]\n",
    "    print(\"Recall of Testing = 1 : {}\".format(thres2 > np.min(pred_y_test)))\n",
    "    print('Testing => auc: {:.6f}'.format(auc_test))\n",
    "    print(\"Threshold is set to {}\".format(thres2))\n",
    "    \n",
    "    y_predict_bin = np.array(pred_y_test >= thres2, dtype=int)\n",
    "    prec_test, recall_test, f1_test = evaluate(y_test, y_predict_bin)\n",
    "    print('prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}'.format(prec_test, recall_test, f1_test))\n",
    "    print(\"Total Positve: {}\".format(np.sum(y_test)))\n",
    "    print(\"Total Candidate: {}\".format(np.sum(pred_y_test >= thres2)))\n",
    "    print('----------------------------------------------')\n",
    "    \n",
    "    ## High Risk\n",
    "    print(\"High Risk Positve: {}\".format(np.sum(y_test[pred_y_test > thres1])))\n",
    "    print(\"High Risk Candidate: {}\".format(np.sum(pred_y_test > thres1)))\n",
    "    print(\"High Risk Prec: {}\".format(np.sum(y_test[np.where(pred_y_test > thres1)])/np.sum(pred_y_test > thres1)))\n",
    "    print('----------------------------------------------')\n",
    "    ## Medium Risk\n",
    "    print(\"Medium Risk Positve: {}\".format(np.sum(y_test[(pred_y_test <= thres1) & (pred_y_test > thres2)])))\n",
    "    print(\"Medium Risk Candidate: {}\".format(np.sum((pred_y_test <= thres1) & (pred_y_test > thres2))))\n",
    "    print(\"Medium Risk Prec: {}\".format(np.sum(y_test[np.where((pred_y_test <= thres1) & (pred_y_test > thres2))])/\\\n",
    "                                                             np.sum((pred_y_test <= thres1) & (pred_y_test > thres2))))\n",
    "    plotting(pred_y_test, y_test, save_path, stat='Test')\n",
    "#     pd.DataFrame([pred_y_test, y_test, testing_announce.tolist()], \n",
    "#                  index=['score', 'label', 'annouce']).T.to_csv('result/Feat_XGB_Testing_res_Part2.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(y_train, pred_y_train, y_test, pred_y_test, save_path='Feat_XGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7.2  Graph Embedding + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= [np.load('../../user_data/CloudMile/data/data_{}_{}.npz'.format(year, month), \n",
    "                     allow_pickle=True) \n",
    "             for year, month in training_date]\n",
    "\n",
    "test_data= [np.load('../../user_data/CloudMile/data/data_{}_{}.npz'.format(year,  month), \n",
    "                    allow_pickle=True) \n",
    "            for year, month in testing_date]\n",
    "\n",
    "X_train = np.concatenate([data['arr_0'] for data in train_data])\n",
    "y_train = np.concatenate([data['arr_1'] for data in train_data])\n",
    "training_announce = np.concatenate([data['arr_2'] for data in train_data])\n",
    "training_FILTER = np.concatenate([data['arr_3'] for data in train_data])\n",
    "\n",
    "X_test = np.concatenate([data['arr_0'] for data in test_data])\n",
    "y_test = np.concatenate([data['arr_1'] for data in test_data])\n",
    "testing_announce = np.concatenate([data['arr_2'] for data in test_data])\n",
    "testing_FILTER = np.concatenate([data['arr_3'] for data in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:,0,:]\n",
    "X_test = X_test[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Announce Extract\n",
    "X_train = X_train[training_announce == 1]\n",
    "y_train = y_train[training_announce == 1]\n",
    "\n",
    "X_test = X_test[testing_announce == 1]\n",
    "y_test = y_test[testing_announce == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model initial & training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(n_estimators=100,\n",
    "                    learning_rate= 0.1,     \n",
    "                    max_depth=5, \n",
    "                    subsample=1,\n",
    "                    gamma=0, \n",
    "                    reg_lambda=1, \n",
    "                    max_delta_step=0,\n",
    "                    colsample_bytree=1, \n",
    "                    min_child_weight=1, \n",
    "                    seed=1000)\n",
    "\n",
    "clf.fit(X=X_train, y=y_train, eval_set=[(X_test, y_test)] , eval_metric='auc', verbose=True)\n",
    "\n",
    "pred_y_train = clf.predict_proba(X_train)[:,1]\n",
    "pred_y_test  = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(y_train, pred_y_train, y_test, pred_y_test, save_path='FeatEmbed_XGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7.3  Graph Embedding + Sequential + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= [np.load('../../user_data/CloudMile/data/data_{}_{}.npz'.format(year, month), \n",
    "                     allow_pickle=True) \n",
    "             for year, month in training_date]\n",
    "\n",
    "test_data= [np.load('../../user_data/CloudMile/data/data_{}_{}.npz'.format(year,  month), \n",
    "                    allow_pickle=True) \n",
    "            for year, month in testing_date]\n",
    "\n",
    "X_train = np.concatenate([data['arr_0'] for data in train_data])\n",
    "y_train = np.concatenate([data['arr_1'] for data in train_data])\n",
    "training_announce = np.concatenate([data['arr_2'] for data in train_data])\n",
    "training_FILTER = np.concatenate([data['arr_3'] for data in train_data])\n",
    "\n",
    "X_test = np.concatenate([data['arr_0'] for data in test_data])\n",
    "y_test = np.concatenate([data['arr_1'] for data in test_data])\n",
    "testing_announce = np.concatenate([data['arr_2'] for data in test_data])\n",
    "testing_FILTER = np.concatenate([data['arr_3'] for data in test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Announce Extract\n",
    "X_train = X_train[training_announce == 1]\n",
    "y_train = y_train[training_announce == 1]\n",
    "\n",
    "X_test = X_test[testing_announce == 1]\n",
    "y_test = y_test[testing_announce == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model initial & training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(n_estimators=100,\n",
    "                    learning_rate= 0.1,     \n",
    "                    max_depth=5, \n",
    "                    subsample=1,\n",
    "                    gamma=0, \n",
    "                    reg_lambda=1, \n",
    "                    max_delta_step=0,\n",
    "                    colsample_bytree=1, \n",
    "                    min_child_weight=1, \n",
    "                    seed=1000)\n",
    "\n",
    "clf.fit(X=X_train, y=y_train, eval_set=[(X_test, y_test)] , eval_metric='auc', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_train = clf.predict_proba(X_train)[:,1]\n",
    "pred_y_test  = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func(y_train, pred_y_train, y_test, pred_y_test, save_path='FeatEmbed-ts_XGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7.4   Sequential + 1D Conv\n",
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(map(lambda ym: (ym, \n",
    "                            np.load('../../user_data/CloudMile/data/data_{}_{}.npz'.format(*ym), allow_pickle=True)), \n",
    "                training_date + testing_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(map(lambda ym: data[ym], training_date))\n",
    "test_data = list(map(lambda ym: data[ym], testing_date))\n",
    "\n",
    "X_train_ = np.concatenate([data['arr_0'] for data in train_data])\n",
    "y_train_ = np.concatenate([data['arr_1'] for data in train_data])\n",
    "training_announce = np.concatenate([data['arr_2'] for data in train_data])\n",
    "training_FILTER = np.concatenate([data['arr_3'] for data in train_data])\n",
    "\n",
    "X_test_ = np.concatenate([data['arr_0'] for data in test_data])\n",
    "y_test_ = np.concatenate([data['arr_1'] for data in test_data])\n",
    "testing_announce = np.concatenate([data['arr_2'] for data in test_data])\n",
    "testing_FILTER = np.concatenate([data['arr_3'] for data in test_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting announced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_[training_announce == 1]\n",
    "y_train = y_train_[training_announce == 1]\n",
    "\n",
    "X_test = X_test_[testing_announce == 1]\n",
    "y_test = y_test_[testing_announce == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magical rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:,:,0] = 0.1 * np.log10(1e4*X_train[:,:,0]**3 + 1e-10) + 1\n",
    "X_train[:,:,2] = 0.1 * np.log10(1e4*X_train[:,:,2]**5 + 1e-10) + 1\n",
    "X_train[:,:,3] = 0.1 * np.log10(1e4*X_train[:,:,3]**2 + 1e-10) + 1\n",
    "X_train[:,:,6] = 0.1 * np.log10(1e0*X_train[:,:,6]**3 + 1e-10) + 1\n",
    "X_train[:,:,7] = 0.1 * np.log10(1e8*X_train[:,:,7]**5 + 1e-10) + 1\n",
    "X_train[:,:,8] = 0.1 * np.log10(1e8*X_train[:,:,8]**3 + 1e-10) + 1\n",
    "X_train[:,:,9] = 0.1 * np.log10(1e8*X_train[:,:,9]**6 + 1e-10) + 1\n",
    "X_train[:,:,10] = 0.1 * np.log10(1e8*X_train[:,:,10]**2 + 1e-10) + 1\n",
    "X_train[:,:,12] = 0.1 * np.log10(1e9*X_train[:,:,12]**3 + 1e-10) + 1\n",
    "X_train[:,:,13] = 0.1 * np.log10(1e9*X_train[:,:,13]**5 + 1e-10) + 1\n",
    "X_train[:,:,14] = 0.1 * np.log10(1e9*X_train[:,:,14]**3.5 + 1e-10) + 1\n",
    "X_train[:,:,15] = 0.1 * np.log10(1e9*X_train[:,:,15]**6 + 1e-10) + 1\n",
    "X_train[:,:,16] = 0.1 * np.log10(1e15*X_train[:,:,16]**3 + 1e-10) + 1\n",
    "X_train[:,:,18] = 0.1 * np.log10(1e15*X_train[:,:,18]**4 + 1e-10) + 1\n",
    "X_train[:,:,20] = 0.1 * np.log10(1e10*X_train[:,:,20]**8 + 1e-10) + 1\n",
    "X_train[:,:,21] = 0.1 * np.log10(1e8*X_train[:,:,21]**3 + 1e-10) + 1\n",
    "X_train[:,:,22] = 0.1 * np.log10(1e20*X_train[:,:,22]**4 + 1e-10) + 1\n",
    "X_train[:,:,23] = 0.1 * np.log10(1e20*X_train[:,:,23]**4 + 1e-10) + 1\n",
    "X_train[:,:,24] = 0.1 * np.log10(1e10*X_train[:,:,24]**5 + 1e-10) + 1\n",
    "X_train[:,:,26] = 0.1 * np.log10(1e10*X_train[:,:,26]**7 + 1e-10) + 1\n",
    "X_train[:,:,27] = 0.1 * np.log10(1e10*X_train[:,:,27]**3 + 1e-10) + 1\n",
    "X_train[:,:,29] = 0.1 * np.log10(1e10*X_train[:,:,29]**7 + 1e-10) + 1\n",
    "X_train[:,:,30] = 0.1 * np.log10(1e10*X_train[:,:,30]**3 + 1e-10) + 1\n",
    "X_train[:,:,32] = 0.1 * np.log10(1e10*X_train[:,:,32]**3 + 1e-10) + 1\n",
    "X_train[:,:,33] = 0.1 * np.log10(1e10*X_train[:,:,33]**2.5 + 1e-10) + 1\n",
    "X_train[:,:,34] = 0.1 * np.log10(1e10*X_train[:,:,34]**3 + 1e-10) + 1\n",
    "X_train[:,:,35] = 0.1 * np.log10(1e20*X_train[:,:,35]**4 + 1e-10) + 1\n",
    "X_train[:,:,36] = 0.1 * np.log10(1e18*X_train[:,:,36]**5 + 1e-10) + 1\n",
    "X_train[:,:,37] = 0.1 * np.log10(1e20*X_train[:,:,37]**4 + 1e-10) + 1\n",
    "X_train[:,:,38] = 0.1 * np.log10(1e20*X_train[:,:,38]**3 + 1e-10) + 1\n",
    "X_train[:,:,39] = 0.1 * np.log10(1e20*X_train[:,:,39]**3 + 1e-10) + 1\n",
    "X_train[:,:,40] = 0.1 * np.log10(1e20*X_train[:,:,40]**3 + 1e-10) + 1\n",
    "X_train[:,:,41] = 0.1 * np.log10(1e20*X_train[:,:,41]**3 + 1e-10) + 1\n",
    "X_train[:,:,42] = 0.1 * np.log10(1e20*X_train[:,:,42]**3 + 1e-10) + 1\n",
    "X_train[:,:,43] = 0.1 * np.log10(1e20*X_train[:,:,43]**3 + 1e-10) + 1\n",
    "X_train[:,:,44] = 0.1 * np.log10(1e20*X_train[:,:,44]**3 + 1e-10) + 1\n",
    "X_train[:,:,45] = 0.1 * np.log10(1e20*X_train[:,:,45]**3 + 1e-10) + 1\n",
    "X_train[:,:,46] = 0.1 * np.log10(1e20*X_train[:,:,46]**3 + 1e-10) + 1\n",
    "X_train[:,:,47] = 0.1 * np.log10(1e20*X_train[:,:,47]**3 + 1e-10) + 1\n",
    "X_train[:,:,51] = 0.1 * np.log10(1e20*X_train[:,:,51]**3 + 1e-10) + 1\n",
    "X_train[:,:,52] = 0.1 * np.log10(1e20*X_train[:,:,52]**3 + 1e-10) + 1\n",
    "X_train[:,:,53] = 0.1 * np.log10(1e20*X_train[:,:,53]**3 + 1e-10) + 1\n",
    "X_train[:,:,54] = 0.1 * np.log10(1e20*X_train[:,:,54]**3 + 1e-10) + 1\n",
    "X_train[:,:,57] = 0.1 * np.log10(1e20*X_train[:,:,57]**20 + 1e-10) + 1\n",
    "X_train[:,:,58] = 0.1 * np.log10(1e20*X_train[:,:,58]**10 + 1e-10) + 1\n",
    "X_train[:,:,59] = 0.1 * np.log10(1e20*X_train[:,:,59]**8 + 1e-10) + 1\n",
    "X_train[:,:,60] = 0.1 * np.log10(1e20*X_train[:,:,60]**6 + 1e-10) + 1\n",
    "X_train[:,:,61] = 0.1 * np.log10(1e20*X_train[:,:,61]**6 + 1e-10) + 1\n",
    "X_train[:,:,62] = 0.1 * np.log10(1e20*X_train[:,:,62]**5 + 1e-10) + 1\n",
    "X_train[:,:,63] = 0.1 * np.log10(1e20*X_train[:,:,63]**3 + 1e-10) + 1\n",
    "X_train[:,:,64] = 0.1 * np.log10(1e20*X_train[:,:,64]**3 + 1e-10) + 1\n",
    "X_train[:,:,65] = 0.1 * np.log10(1e20*X_train[:,:,65]**3 + 1e-10) + 1\n",
    "X_train[:,:,66] = 0.1 * np.log10(1e20*X_train[:,:,66]**3 + 1e-10) + 1\n",
    "X_train[:,:,67] = 0.1 * np.log10(1e20*X_train[:,:,67]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,68] = 0.1 * np.log10(1e20*X_train[:,:,68]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,69] = 0.1 * np.log10(1e20*X_train[:,:,69]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,70] = 0.1 * np.log10(1e20*X_train[:,:,70]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,71] = 0.1 * np.log10(1e20*X_train[:,:,71]**3 + 1e-10) + 1\n",
    "X_train[:,:,72] = 0.1 * np.log10(1e20*X_train[:,:,72]**3 + 1e-10) + 1\n",
    "X_train[:,:,73] = 0.1 * np.log10(1e20*X_train[:,:,73]**3 + 1e-10) + 1\n",
    "X_train[:,:,74] = 0.1 * np.log10(1e20*X_train[:,:,74]**3 + 1e-10) + 1\n",
    "X_train[:,:,75] = 0.1 * np.log10(1e20*X_train[:,:,75]**3 + 1e-10) + 1\n",
    "X_train[:,:,76] = 0.1 * np.log10(1e20*X_train[:,:,76]**3 + 1e-10) + 1\n",
    "X_train[:,:,77] = 0.1 * np.log10(1e15*X_train[:,:,77]**8 + 1e-10) + 1\n",
    "X_train[:,:,78] = 0.1 * np.log10(1e15*X_train[:,:,78]**8 + 1e-10) + 1\n",
    "X_train[:,:,79] = 0.1 * np.log10(1e20*X_train[:,:,79]**5 + 1e-10) + 1\n",
    "X_train[:,:,80] = 0.1 * np.log10(1e20*X_train[:,:,80]**6 + 1e-10) + 1\n",
    "X_train[:,:,81] = 0.1 * np.log10(1e20*X_train[:,:,81]**8 + 1e-10) + 1\n",
    "X_train[:,:,82] = 0.1 * np.log10(1e20*X_train[:,:,82]**10 + 1e-10) + 1\n",
    "X_train[:,:,83] = 0.1 * np.log10(1e20*X_train[:,:,83]**9.5 + 1e-10) + 1\n",
    "X_train[:,:,84] = 0.1 * np.log10(1e20*X_train[:,:,84]**9.5 + 1e-10) + 1\n",
    "X_train[:,:,85] = 0.1 * np.log10(1e20*X_train[:,:,85]**3 + 1e-10) + 1\n",
    "X_train[:,:,86] = 0.1 * np.log10(1e20*X_train[:,:,86]**13 + 1e-10) + 1\n",
    "X_train[:,:,87] = 0.1 * np.log10(1e20*X_train[:,:,87]**10 + 1e-10) + 1\n",
    "X_train[:,:,88] = 0.1 * np.log10(1e20*X_train[:,:,88]**9 + 1e-10) + 1\n",
    "X_train[:,:,89] = 0.1 * np.log10(1e20*X_train[:,:,89]**8 + 1e-10) + 1\n",
    "X_train[:,:,91] = X_train[:,:,91] ** 5\n",
    "X_train[:,:,93] = X_train[:,:,93] ** 2\n",
    "X_train[:,:,94] = X_train[:,:,94] ** 2\n",
    "X_train[:,:,96] = 0.1 * np.log10(1e20*X_train[:,:,96]**4 + 1e-10) + 1\n",
    "X_train[:,:,97] = 0.1 * np.log10(1e20*X_train[:,:,97]**9 + 1e-10) + 1\n",
    "X_train[:,:,98] = 0.1 * np.log10(1e20*X_train[:,:,98]**8 + 1e-10) + 1\n",
    "X_train[:,:,99] = 0.1 * np.log10(1e20*X_train[:,:,99]**7 + 1e-10) + 1\n",
    "X_train[:,:,100] = 0.1 * np.log10(1e20*X_train[:,:,100]**7 + 1e-10) + 1\n",
    "X_train[:,:,101] = 0.1 * np.log10(1e20*X_train[:,:,101]**5 + 1e-10) + 1\n",
    "X_train[:,:,102] = 0.1 * np.log10(1e20*X_train[:,:,102]**4 + 1e-10) + 1\n",
    "X_train[:,:,103] = 0.1 * np.log10(1e20*X_train[:,:,103]**4 + 1e-10) + 1\n",
    "X_train[:,:,104] = 0.1 * np.log10(1e20*X_train[:,:,104]**10 + 1e-10) + 1\n",
    "X_train[:,:,106] = 0.1 * np.log10(1e20*X_train[:,:,106]**4 + 1e-10) + 1\n",
    "X_train[:,:,107] = 0.1 * np.log10(1e20*X_train[:,:,107]**8 + 1e-10) + 1\n",
    "X_train[:,:,108] = 0.1 * np.log10(1e20*X_train[:,:,108]**7 + 1e-10) + 1\n",
    "X_train[:,:,109] = 0.1 * np.log10(1e20*X_train[:,:,109]**7 + 1e-10) + 1\n",
    "X_train[:,:,110] = 0.1 * np.log10(1e20*X_train[:,:,110]**7 + 1e-10) + 1\n",
    "X_train[:,:,111] = 0.1 * np.log10(1e20*X_train[:,:,111]**6 + 1e-10) + 1\n",
    "X_train[:,:,112] = 0.1 * np.log10(1e20*X_train[:,:,112]**5 + 1e-10) + 1\n",
    "X_train[:,:,113] = 0.1 * np.log10(1e20*X_train[:,:,113]**5 + 1e-10) + 1\n",
    "X_train[:,:,114] = 0.1 * np.log10(1e20*X_train[:,:,114]**8 + 1e-10) + 1\n",
    "X_train[:,:,115] = 0.1 * np.log10(1e20*X_train[:,:,115]**7 + 1e-10) + 1\n",
    "X_train[:,:,116] = 0.1 * np.log10(1e20*X_train[:,:,116]**6 + 1e-10) + 1\n",
    "X_train[:,:,117] = 0.1 * np.log10(1e20*X_train[:,:,117]**6 + 1e-10) + 1\n",
    "X_train[:,:,118] = 0.1 * np.log10(1e20*X_train[:,:,118]**5.5 + 1e-10) + 1\n",
    "X_train[:,:,119] = 0.1 * np.log10(1e20*X_train[:,:,119]**4 + 1e-10) + 1\n",
    "X_train[:,:,124] = 0.1 * np.log10(1e20*X_train[:,:,124]**6 + 1e-10) + 1\n",
    "X_train[:,:,125] = 0.1 * np.log10(1e20*X_train[:,:,125]**7.5 + 1e-10) + 1\n",
    "X_train[:,:,126] = 0.1 * np.log10(1e20*X_train[:,:,126]**11 + 1e-10) + 1\n",
    "X_train[:,:,127] = 0.1 * np.log10(1e20*X_train[:,:,127]**8 + 1e-10) + 1\n",
    "X_train[:,:,128] = 0.1 * np.log10(1e20*X_train[:,:,128]**8 + 1e-10) + 1\n",
    "X_train[:,:,129] = 0.1 * np.log10(1e20*X_train[:,:,129]**8 + 1e-10) + 1\n",
    "X_train[:,:,130] = 0.1 * np.log10(1e20*X_train[:,:,130]**8 + 1e-10) + 1\n",
    "X_train[:,:,132] = 0.1 * np.log10(1e12*X_train[:,:,132]**12 + 1e-10) + 1\n",
    "X_train[:,:,133] = 0.1 * np.log10(1e20*X_train[:,:,133]**5.5 + 1e-10) + 1\n",
    "X_train[:,:,140] = 0.1 * np.log10(1e20*X_train[:,:,140]**4 + 1e-10) + 1\n",
    "X_train[:,:,141] = 0.1 * np.log10(1e20*X_train[:,:,141]**4 + 1e-10) + 1\n",
    "X_train[:,:,142] = 0.1 * np.log10(1e20*X_train[:,:,142]**4 + 1e-10) + 1\n",
    "X_train[:,:,143] = 0.1 * np.log10(1e20*X_train[:,:,143]**4 + 1e-10) + 1\n",
    "X_train[:,:,144] = 0.1 * np.log10(1e20*X_train[:,:,144]**10 + 1e-10) + 1\n",
    "X_train[:,:,146] = 0.1 * np.log10(1e20*X_train[:,:,146]**10 + 1e-10) + 1\n",
    "X_train[:,:,147] = 0.1 * np.log10(1e20*X_train[:,:,147]**9 + 1e-10) + 1\n",
    "X_train[:,:,148] = 0.1 * np.log10(1e20*X_train[:,:,148]**8.5 + 1e-10) + 1\n",
    "X_train[:,:,149] = 0.1 * np.log10(1e20*X_train[:,:,149]**9 + 1e-10) + 1\n",
    "X_train[:,:,150] = 0.1 * np.log10(1e20*X_train[:,:,150]**7.5 + 1e-10) + 1\n",
    "X_train[:,:,151] = 0.1 * np.log10(1e20*X_train[:,:,151]**12 + 1e-10) + 1\n",
    "X_train[:,:,152] = 0.1 * np.log10(1e20*X_train[:,:,152]**20 + 1e-10) + 1\n",
    "X_train[:,:,153] = 0.1 * np.log10(1e20*X_train[:,:,153]**11 + 1e-10) + 1\n",
    "X_train[:,:,154] = 0.1 * np.log10(1e20*X_train[:,:,154]**20 + 1e-10) + 1\n",
    "X_train[:,:,155] = 0.1 * np.log10(1e20*X_train[:,:,155]**16 + 1e-10) + 1\n",
    "X_train[:,:,156] = 0.1 * np.log10(1e20*X_train[:,:,156]**10 + 1e-10) + 1\n",
    "X_train[:,:,157] = 0.1 * np.log10(1e20*X_train[:,:,157]**11 + 1e-10) + 1\n",
    "X_train[:,:,158] = 0.1 * np.log10(1e20*X_train[:,:,158]**9 + 1e-10) + 1\n",
    "X_train[:,:,159] = 0.1 * np.log10(1e20*X_train[:,:,159]**3.5 + 1e-10) + 1\n",
    "X_train[:,:,160] = 0.1 * np.log10(1e20*X_train[:,:,160]**11 + 1e-10) + 1\n",
    "X_train[:,:,166] = 0.1 * np.log10(1e20*X_train[:,:,166]**11 + 1e-10) + 1\n",
    "X_train[:,:,168] = 0.1 * np.log10(1e20*X_train[:,:,168]**14 + 1e-10) + 1\n",
    "X_train[:,:,173] = 0.1 * np.log10(1e30*X_train[:,:,173]**7 + 1e-10) + 1\n",
    "X_train[:,:,174] = 0.1 * np.log10(1e30*X_train[:,:,174]**4 + 1e-10) + 1\n",
    "X_train[:,:,178] = 0.1 * np.log10(1e30*X_train[:,:,178]**4 + 1e-10) + 1\n",
    "X_train[:,:,179] = 0.1 * np.log10(1e30*X_train[:,:,179]**4 + 1e-10) + 1\n",
    "X_train[:,:,181] = 0.1 * np.log10(1e30*X_train[:,:,181]**7 + 1e-10) + 1\n",
    "X_train[:,:,182] = 0.1 * np.log10(1e30*X_train[:,:,182]**2 + 1e-10) + 1\n",
    "X_train[:,:,183] = 0.1 * np.log10(1e30*X_train[:,:,183]**2 + 1e-10) + 1\n",
    "X_train[:,:,185] = 0.1 * np.log10(1e30*X_train[:,:,185]**2 + 1e-10) + 1\n",
    "X_train[:,:,186] = 0.1 * np.log10(1e30*X_train[:,:,186]**2 + 1e-10) + 1\n",
    "X_train[:,:,187] = 0.1 * np.log10(1e30*X_train[:,:,187]**2 + 1e-10) + 1\n",
    "X_train[:,:,190] = 0.1 * np.log10(1e10*X_train[:,:,190]**4 + 1e-10) + 1\n",
    "X_train[:,:,191] = 0.1 * np.log10(1e20*X_train[:,:,191]**6 + 1e-10) + 1\n",
    "X_train[:,:,192] = 0.1 * np.log10(1e20*X_train[:,:,192]**4 + 1e-10) + 1\n",
    "X_train[:,:,195] = 0.1 * np.log10(1e20*X_train[:,:,195]**4 + 1e-10) + 1\n",
    "X_train[:,:,196] = 0.1 * np.log10(1e20*X_train[:,:,196]**6 + 1e-10) + 1\n",
    "X_train[:,:,197] = 0.1 * np.log10(1e20*X_train[:,:,197]**4 + 1e-10) + 1\n",
    "X_train[:,:,198] = 0.1 * np.log10(1e20*X_train[:,:,198]**6 + 1e-10) + 1\n",
    "X_train[:,:,200] = 0.1 * np.log10(1e20*X_train[:,:,200]**6 + 1e-10) + 1\n",
    "X_train[:,:,201] = 0.1 * np.log10(1e20*X_train[:,:,201]**5 + 1e-10) + 1\n",
    "X_train[:,:,202] = 0.1 * np.log10(1e20*X_train[:,:,202]**5 + 1e-10) + 1\n",
    "X_train[:,:,203] = 0.1 * np.log10(1e20*X_train[:,:,203]**5 + 1e-10) + 1\n",
    "X_train[:,:,204] = 0.1 * np.log10(1e30*X_train[:,:,204]**3 + 1e-10) + 1\n",
    "X_train[:,:,205] = 0.1 * np.log10(1e30*X_train[:,:,205]**3 + 1e-10) + 1\n",
    "X_train[:,:,206] = 0.1 * np.log10(1e20*X_train[:,:,206]**7 + 1e-10) + 1\n",
    "X_train[:,:,207] = 0.1 * np.log10(1e30*X_train[:,:,207]**2 + 1e-10) + 1\n",
    "X_train[:,:,208] = 0.1 * np.log10(1e30*X_train[:,:,208]**2 + 1e-10) + 1\n",
    "X_train[:,:,209] = 0.1 * np.log10(1e30*X_train[:,:,209]**2 + 1e-10) + 1\n",
    "X_train[:,:,213] = 0.1 * np.log10(1e30*X_train[:,:,213]**2 + 1e-10) + 1\n",
    "X_train[:,:,214] = 0.1 * np.log10(1e7*X_train[:,:,214]**2 + 1e-10) + 1\n",
    "X_train[:,:,215] = 0.1 * np.log10(1e15*X_train[:,:,215]**2 + 1e-10) + 1\n",
    "X_train[:,:,216] = 0.1 * np.log10(1e15*X_train[:,:,216]**2 + 1e-10) + 1\n",
    "X_train[:,:,217] = 0.1 * np.log10(1e15*X_train[:,:,217]**2 + 1e-10) + 1\n",
    "X_train[:,:,218] = 0.1 * np.log10(1e15*X_train[:,:,218]**2 + 1e-10) + 1\n",
    "X_train[:,:,220] = 0.1 * np.log10(1e15*X_train[:,:,220]**2 + 1e-10) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get Feat\n",
    "X_train = np.dstack([X_train[:,:,:314], X_train[:,:,-14:]])\n",
    "X_test = np.dstack([X_test[:,:,:314], X_test[:,:,-14:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Numpy tensor into PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(X_train)):\n",
    "    train_data.append((X_train[i], y_train[i]))\n",
    "    \n",
    "test_data = []\n",
    "for i in range(len(X_test)):\n",
    "    test_data.append((X_test[i], y_test[i]))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "### Classifier / Focal Loss initialization\n",
    "Notice that the dimensions per layer are recommended to set to a smaller value due to the risk of overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ConvNet(fc_dims=[128, 64], in_dim=X_train.shape[2], out_dim=2).cuda()\n",
    "\n",
    "focal_loss = FocalLoss2(alpha, gamma_pos, gamma_neg)\n",
    "if optim_type == \"adam\":\n",
    "    optim_clsfr = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), \n",
    "                             lr=learn_rate)\n",
    "else:\n",
    "    optim_clsfr = RangerLars(filter(lambda p: p.requires_grad, classifier.parameters()),\n",
    "                             lr=learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VAT Loss initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vat_loss2 = VATLoss2(xi=vat_xi, eps_pos=vat_eps_pos, eps_neg=vat_eps_neg, ip=vat_ip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameter Setting ----------------------------------------------------------------------')\n",
    "print('Model = VAT + Conv1D')\n",
    "print('conv1d use activation = {}'.format(True))\n",
    "print('graph_emdeding = Heter_AutoEncoder')\n",
    "print('alpha = {}'.format(alpha))\n",
    "print('gamma_pos = {}'.format(gamma_pos))\n",
    "print('gamma_neg = {}'.format(gamma_neg))\n",
    "print('learn_rate = {}'.format(learn_rate))\n",
    "print('train_batch_size = {}'.format(train_batch_size))\n",
    "print('test_batch_size = {}'.format(test_batch_size))\n",
    "print('max_epochs = {}'.format(max_epochs))\n",
    "print('vat_xi = {}'.format(vat_xi))\n",
    "print('vat_eps_pos = {}'.format(vat_eps_pos))\n",
    "print('vat_eps_neg = {}'.format(vat_eps_neg))\n",
    "print('vat_ip = {}'.format(vat_ip))\n",
    "print('optim_type = {}'.format(optim_type))\n",
    "print('weight_decay = {}'.format(weight_decay))\n",
    "print('lambda_l1 = {}'.format(lambda_l1))\n",
    "print('\\n')\n",
    "\n",
    "train_history_loss = []\n",
    "train_history_auc = []\n",
    "max_f1score = 0\n",
    "for epoch in range(max_epochs):\n",
    "    print('Epoch {} -------------------------------------------------------------------------'.format(epoch))\n",
    "    \n",
    "    #\n",
    "    # Training\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    classifier.train()\n",
    "    label_train, pred_y_train, clsf_loss_train, clsf_loss_pos_train, \\\n",
    "        clsf_loss_neg_train, vat_loss_train = train(epoch, train_dataloader, \n",
    "                                                    clip_grad_norm=grad_clip, lambda_l1=lambda_l1)\n",
    "    \n",
    "    auc_train = roc_auc_score(label_train, pred_y_train)\n",
    "    train_history_loss.append(clsf_loss_train)\n",
    "    train_history_auc.append(auc_train)\n",
    "    print('    Training => auc: {:.6f}, clsf_pos: {}, clsf_neg: {}, vat_loss: {}'.\n",
    "          format(auc_train, clsf_loss_pos_train, clsf_loss_neg_train, vat_loss_train))\n",
    "    thres = np.min(pred_y_train[label_train==1]) - 1e-6\n",
    "    print(\"                Threshold is set to {}\".format(thres))\n",
    "    print(\"                Min. Probailities on train is {}\".format(np.min(pred_y_train)))\n",
    "    \n",
    "    y_predict_bin = np.array(pred_y_train > thres, dtype=int)\n",
    "    prec_train, recall_train, f1_train = evaluate(label_train, y_predict_bin)\n",
    "    print('                prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}'.\n",
    "          format(prec_train, recall_train, f1_train))\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        #\n",
    "        # Testing\n",
    "        # ------------------------------------------------------------------------------------        \n",
    "        with torch.no_grad():\n",
    "            classifier.eval()\n",
    "            label_test, pred_y_test, clsf_loss_test, clsf_loss_pos_test, clsf_loss_neg_test = infer(test_dataloader)    \n",
    "        \n",
    "        auc = roc_auc_score(label_test, pred_y_test)\n",
    "        \n",
    "        print(\"            Min. Probailities on test set with label 1: {}\".format(np.min(pred_y_test[label_test==1])))\n",
    "        print(\"            Min. Probailities on test set             : {}\".format(np.min(pred_y_test)))\n",
    "        \n",
    "        y_predict_bin = np.array(pred_y_test > thres, dtype=int)\n",
    "        prec, recall_bytrain, f1_bytrain = evaluate(label_test, y_predict_bin)\n",
    "        print('    Testing by train ==> auc: {:.6f}, prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}, clsf_loss: {}'.\n",
    "              format(auc, prec, recall_bytrain, f1_bytrain, clsf_loss_test))\n",
    "        \n",
    "        y_predict_bin = np.array(pred_y_test >= np.min(pred_y_test[label_test==1]), dtype=int)\n",
    "        prec, recall_bytest, f1_bytest = evaluate(label_test, y_predict_bin)\n",
    "        print('    Testing by test ==> auc: {:.6f}, prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}, clsf_loss: {}'.\n",
    "              format(auc, prec, recall_bytest, f1_bytest, clsf_loss_test))\n",
    "        \n",
    "        if clsf_loss_pos_train < 0.005 and f1_bytrain > max_f1score and recall_bytrain == 1:\n",
    "            max_f1score = f1_bytrain\n",
    "            torch.save({'epoch': epoch,\n",
    "                        'model_state_dict': classifier.state_dict(),\n",
    "                        'optimizer_state_dict': optim_clsfr.state_dict(),\n",
    "                        'loss': focal_loss,\n",
    "                       }, \n",
    "                       '../../user_data/CloudMile/data/saved_models/' + \\\n",
    "                       'Feat-ts-Conv1Dsmall_v2_eps{}{}_focal{}{}_{}_lr{}_epoch{}_Part2_{}'.\n",
    "                       format(int(-math.log10(vat_eps_pos)), \n",
    "                              int(-math.log10(vat_eps_neg)), \n",
    "                              gamma_pos, gamma_neg, \n",
    "                              optim_type, int(-math.log10(learn_rate)), max_epochs, f1_bytrain))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '{model path}'\n",
    "performance(PATH, 'Feat-Golden-LastSoftmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  7.5   Proposed Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_[training_announce == 1]\n",
    "y_train = y_train_[training_announce == 1]\n",
    "\n",
    "X_test = X_test_[testing_announce == 1]\n",
    "y_test = y_test_[testing_announce == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magical rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:,:,0] = 0.1 * np.log10(1e4*X_train[:,:,0]**3 + 1e-10) + 1\n",
    "X_train[:,:,2] = 0.1 * np.log10(1e4*X_train[:,:,2]**5 + 1e-10) + 1\n",
    "X_train[:,:,3] = 0.1 * np.log10(1e4*X_train[:,:,3]**2 + 1e-10) + 1\n",
    "X_train[:,:,6] = 0.1 * np.log10(1e0*X_train[:,:,6]**3 + 1e-10) + 1\n",
    "X_train[:,:,7] = 0.1 * np.log10(1e8*X_train[:,:,7]**5 + 1e-10) + 1\n",
    "X_train[:,:,8] = 0.1 * np.log10(1e8*X_train[:,:,8]**3 + 1e-10) + 1\n",
    "X_train[:,:,9] = 0.1 * np.log10(1e8*X_train[:,:,9]**6 + 1e-10) + 1\n",
    "X_train[:,:,10] = 0.1 * np.log10(1e8*X_train[:,:,10]**2 + 1e-10) + 1\n",
    "X_train[:,:,12] = 0.1 * np.log10(1e9*X_train[:,:,12]**3 + 1e-10) + 1\n",
    "X_train[:,:,13] = 0.1 * np.log10(1e9*X_train[:,:,13]**5 + 1e-10) + 1\n",
    "X_train[:,:,14] = 0.1 * np.log10(1e9*X_train[:,:,14]**3.5 + 1e-10) + 1\n",
    "X_train[:,:,15] = 0.1 * np.log10(1e9*X_train[:,:,15]**6 + 1e-10) + 1\n",
    "X_train[:,:,16] = 0.1 * np.log10(1e15*X_train[:,:,16]**3 + 1e-10) + 1\n",
    "X_train[:,:,18] = 0.1 * np.log10(1e15*X_train[:,:,18]**4 + 1e-10) + 1\n",
    "X_train[:,:,20] = 0.1 * np.log10(1e10*X_train[:,:,20]**8 + 1e-10) + 1\n",
    "X_train[:,:,21] = 0.1 * np.log10(1e8*X_train[:,:,21]**3 + 1e-10) + 1\n",
    "X_train[:,:,22] = 0.1 * np.log10(1e20*X_train[:,:,22]**4 + 1e-10) + 1\n",
    "X_train[:,:,23] = 0.1 * np.log10(1e20*X_train[:,:,23]**4 + 1e-10) + 1\n",
    "X_train[:,:,24] = 0.1 * np.log10(1e10*X_train[:,:,24]**5 + 1e-10) + 1\n",
    "X_train[:,:,26] = 0.1 * np.log10(1e10*X_train[:,:,26]**7 + 1e-10) + 1\n",
    "X_train[:,:,27] = 0.1 * np.log10(1e10*X_train[:,:,27]**3 + 1e-10) + 1\n",
    "X_train[:,:,29] = 0.1 * np.log10(1e10*X_train[:,:,29]**7 + 1e-10) + 1\n",
    "X_train[:,:,30] = 0.1 * np.log10(1e10*X_train[:,:,30]**3 + 1e-10) + 1\n",
    "X_train[:,:,32] = 0.1 * np.log10(1e10*X_train[:,:,32]**3 + 1e-10) + 1\n",
    "X_train[:,:,33] = 0.1 * np.log10(1e10*X_train[:,:,33]**2.5 + 1e-10) + 1\n",
    "X_train[:,:,34] = 0.1 * np.log10(1e10*X_train[:,:,34]**3 + 1e-10) + 1\n",
    "X_train[:,:,35] = 0.1 * np.log10(1e20*X_train[:,:,35]**4 + 1e-10) + 1\n",
    "X_train[:,:,36] = 0.1 * np.log10(1e18*X_train[:,:,36]**5 + 1e-10) + 1\n",
    "X_train[:,:,37] = 0.1 * np.log10(1e20*X_train[:,:,37]**4 + 1e-10) + 1\n",
    "X_train[:,:,38] = 0.1 * np.log10(1e20*X_train[:,:,38]**3 + 1e-10) + 1\n",
    "X_train[:,:,39] = 0.1 * np.log10(1e20*X_train[:,:,39]**3 + 1e-10) + 1\n",
    "X_train[:,:,40] = 0.1 * np.log10(1e20*X_train[:,:,40]**3 + 1e-10) + 1\n",
    "X_train[:,:,41] = 0.1 * np.log10(1e20*X_train[:,:,41]**3 + 1e-10) + 1\n",
    "X_train[:,:,42] = 0.1 * np.log10(1e20*X_train[:,:,42]**3 + 1e-10) + 1\n",
    "X_train[:,:,43] = 0.1 * np.log10(1e20*X_train[:,:,43]**3 + 1e-10) + 1\n",
    "X_train[:,:,44] = 0.1 * np.log10(1e20*X_train[:,:,44]**3 + 1e-10) + 1\n",
    "X_train[:,:,45] = 0.1 * np.log10(1e20*X_train[:,:,45]**3 + 1e-10) + 1\n",
    "X_train[:,:,46] = 0.1 * np.log10(1e20*X_train[:,:,46]**3 + 1e-10) + 1\n",
    "X_train[:,:,47] = 0.1 * np.log10(1e20*X_train[:,:,47]**3 + 1e-10) + 1\n",
    "X_train[:,:,51] = 0.1 * np.log10(1e20*X_train[:,:,51]**3 + 1e-10) + 1\n",
    "X_train[:,:,52] = 0.1 * np.log10(1e20*X_train[:,:,52]**3 + 1e-10) + 1\n",
    "X_train[:,:,53] = 0.1 * np.log10(1e20*X_train[:,:,53]**3 + 1e-10) + 1\n",
    "X_train[:,:,54] = 0.1 * np.log10(1e20*X_train[:,:,54]**3 + 1e-10) + 1\n",
    "X_train[:,:,57] = 0.1 * np.log10(1e20*X_train[:,:,57]**20 + 1e-10) + 1\n",
    "X_train[:,:,58] = 0.1 * np.log10(1e20*X_train[:,:,58]**10 + 1e-10) + 1\n",
    "X_train[:,:,59] = 0.1 * np.log10(1e20*X_train[:,:,59]**8 + 1e-10) + 1\n",
    "X_train[:,:,60] = 0.1 * np.log10(1e20*X_train[:,:,60]**6 + 1e-10) + 1\n",
    "X_train[:,:,61] = 0.1 * np.log10(1e20*X_train[:,:,61]**6 + 1e-10) + 1\n",
    "X_train[:,:,62] = 0.1 * np.log10(1e20*X_train[:,:,62]**5 + 1e-10) + 1\n",
    "X_train[:,:,63] = 0.1 * np.log10(1e20*X_train[:,:,63]**3 + 1e-10) + 1\n",
    "X_train[:,:,64] = 0.1 * np.log10(1e20*X_train[:,:,64]**3 + 1e-10) + 1\n",
    "X_train[:,:,65] = 0.1 * np.log10(1e20*X_train[:,:,65]**3 + 1e-10) + 1\n",
    "X_train[:,:,66] = 0.1 * np.log10(1e20*X_train[:,:,66]**3 + 1e-10) + 1\n",
    "X_train[:,:,67] = 0.1 * np.log10(1e20*X_train[:,:,67]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,68] = 0.1 * np.log10(1e20*X_train[:,:,68]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,69] = 0.1 * np.log10(1e20*X_train[:,:,69]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,70] = 0.1 * np.log10(1e20*X_train[:,:,70]**1.5 + 1e-10) + 1\n",
    "X_train[:,:,71] = 0.1 * np.log10(1e20*X_train[:,:,71]**3 + 1e-10) + 1\n",
    "X_train[:,:,72] = 0.1 * np.log10(1e20*X_train[:,:,72]**3 + 1e-10) + 1\n",
    "X_train[:,:,73] = 0.1 * np.log10(1e20*X_train[:,:,73]**3 + 1e-10) + 1\n",
    "X_train[:,:,74] = 0.1 * np.log10(1e20*X_train[:,:,74]**3 + 1e-10) + 1\n",
    "X_train[:,:,75] = 0.1 * np.log10(1e20*X_train[:,:,75]**3 + 1e-10) + 1\n",
    "X_train[:,:,76] = 0.1 * np.log10(1e20*X_train[:,:,76]**3 + 1e-10) + 1\n",
    "X_train[:,:,77] = 0.1 * np.log10(1e15*X_train[:,:,77]**8 + 1e-10) + 1\n",
    "X_train[:,:,78] = 0.1 * np.log10(1e15*X_train[:,:,78]**8 + 1e-10) + 1\n",
    "X_train[:,:,79] = 0.1 * np.log10(1e20*X_train[:,:,79]**5 + 1e-10) + 1\n",
    "X_train[:,:,80] = 0.1 * np.log10(1e20*X_train[:,:,80]**6 + 1e-10) + 1\n",
    "X_train[:,:,81] = 0.1 * np.log10(1e20*X_train[:,:,81]**8 + 1e-10) + 1\n",
    "X_train[:,:,82] = 0.1 * np.log10(1e20*X_train[:,:,82]**10 + 1e-10) + 1\n",
    "X_train[:,:,83] = 0.1 * np.log10(1e20*X_train[:,:,83]**9.5 + 1e-10) + 1\n",
    "X_train[:,:,84] = 0.1 * np.log10(1e20*X_train[:,:,84]**9.5 + 1e-10) + 1\n",
    "X_train[:,:,85] = 0.1 * np.log10(1e20*X_train[:,:,85]**3 + 1e-10) + 1\n",
    "X_train[:,:,86] = 0.1 * np.log10(1e20*X_train[:,:,86]**13 + 1e-10) + 1\n",
    "X_train[:,:,87] = 0.1 * np.log10(1e20*X_train[:,:,87]**10 + 1e-10) + 1\n",
    "X_train[:,:,88] = 0.1 * np.log10(1e20*X_train[:,:,88]**9 + 1e-10) + 1\n",
    "X_train[:,:,89] = 0.1 * np.log10(1e20*X_train[:,:,89]**8 + 1e-10) + 1\n",
    "X_train[:,:,91] = X_train[:,:,91] ** 5\n",
    "X_train[:,:,93] = X_train[:,:,93] ** 2\n",
    "X_train[:,:,94] = X_train[:,:,94] ** 2\n",
    "X_train[:,:,96] = 0.1 * np.log10(1e20*X_train[:,:,96]**4 + 1e-10) + 1\n",
    "X_train[:,:,97] = 0.1 * np.log10(1e20*X_train[:,:,97]**9 + 1e-10) + 1\n",
    "X_train[:,:,98] = 0.1 * np.log10(1e20*X_train[:,:,98]**8 + 1e-10) + 1\n",
    "X_train[:,:,99] = 0.1 * np.log10(1e20*X_train[:,:,99]**7 + 1e-10) + 1\n",
    "X_train[:,:,100] = 0.1 * np.log10(1e20*X_train[:,:,100]**7 + 1e-10) + 1\n",
    "X_train[:,:,101] = 0.1 * np.log10(1e20*X_train[:,:,101]**5 + 1e-10) + 1\n",
    "X_train[:,:,102] = 0.1 * np.log10(1e20*X_train[:,:,102]**4 + 1e-10) + 1\n",
    "X_train[:,:,103] = 0.1 * np.log10(1e20*X_train[:,:,103]**4 + 1e-10) + 1\n",
    "X_train[:,:,104] = 0.1 * np.log10(1e20*X_train[:,:,104]**10 + 1e-10) + 1\n",
    "X_train[:,:,106] = 0.1 * np.log10(1e20*X_train[:,:,106]**4 + 1e-10) + 1\n",
    "X_train[:,:,107] = 0.1 * np.log10(1e20*X_train[:,:,107]**8 + 1e-10) + 1\n",
    "X_train[:,:,108] = 0.1 * np.log10(1e20*X_train[:,:,108]**7 + 1e-10) + 1\n",
    "X_train[:,:,109] = 0.1 * np.log10(1e20*X_train[:,:,109]**7 + 1e-10) + 1\n",
    "X_train[:,:,110] = 0.1 * np.log10(1e20*X_train[:,:,110]**7 + 1e-10) + 1\n",
    "X_train[:,:,111] = 0.1 * np.log10(1e20*X_train[:,:,111]**6 + 1e-10) + 1\n",
    "X_train[:,:,112] = 0.1 * np.log10(1e20*X_train[:,:,112]**5 + 1e-10) + 1\n",
    "X_train[:,:,113] = 0.1 * np.log10(1e20*X_train[:,:,113]**5 + 1e-10) + 1\n",
    "X_train[:,:,114] = 0.1 * np.log10(1e20*X_train[:,:,114]**8 + 1e-10) + 1\n",
    "X_train[:,:,115] = 0.1 * np.log10(1e20*X_train[:,:,115]**7 + 1e-10) + 1\n",
    "X_train[:,:,116] = 0.1 * np.log10(1e20*X_train[:,:,116]**6 + 1e-10) + 1\n",
    "X_train[:,:,117] = 0.1 * np.log10(1e20*X_train[:,:,117]**6 + 1e-10) + 1\n",
    "X_train[:,:,118] = 0.1 * np.log10(1e20*X_train[:,:,118]**5.5 + 1e-10) + 1\n",
    "X_train[:,:,119] = 0.1 * np.log10(1e20*X_train[:,:,119]**4 + 1e-10) + 1\n",
    "X_train[:,:,124] = 0.1 * np.log10(1e20*X_train[:,:,124]**6 + 1e-10) + 1\n",
    "X_train[:,:,125] = 0.1 * np.log10(1e20*X_train[:,:,125]**7.5 + 1e-10) + 1\n",
    "X_train[:,:,126] = 0.1 * np.log10(1e20*X_train[:,:,126]**11 + 1e-10) + 1\n",
    "X_train[:,:,127] = 0.1 * np.log10(1e20*X_train[:,:,127]**8 + 1e-10) + 1\n",
    "X_train[:,:,128] = 0.1 * np.log10(1e20*X_train[:,:,128]**8 + 1e-10) + 1\n",
    "X_train[:,:,129] = 0.1 * np.log10(1e20*X_train[:,:,129]**8 + 1e-10) + 1\n",
    "X_train[:,:,130] = 0.1 * np.log10(1e20*X_train[:,:,130]**8 + 1e-10) + 1\n",
    "X_train[:,:,132] = 0.1 * np.log10(1e12*X_train[:,:,132]**12 + 1e-10) + 1\n",
    "X_train[:,:,133] = 0.1 * np.log10(1e20*X_train[:,:,133]**5.5 + 1e-10) + 1\n",
    "X_train[:,:,140] = 0.1 * np.log10(1e20*X_train[:,:,140]**4 + 1e-10) + 1\n",
    "X_train[:,:,141] = 0.1 * np.log10(1e20*X_train[:,:,141]**4 + 1e-10) + 1\n",
    "X_train[:,:,142] = 0.1 * np.log10(1e20*X_train[:,:,142]**4 + 1e-10) + 1\n",
    "X_train[:,:,143] = 0.1 * np.log10(1e20*X_train[:,:,143]**4 + 1e-10) + 1\n",
    "X_train[:,:,144] = 0.1 * np.log10(1e20*X_train[:,:,144]**10 + 1e-10) + 1\n",
    "X_train[:,:,146] = 0.1 * np.log10(1e20*X_train[:,:,146]**10 + 1e-10) + 1\n",
    "X_train[:,:,147] = 0.1 * np.log10(1e20*X_train[:,:,147]**9 + 1e-10) + 1\n",
    "X_train[:,:,148] = 0.1 * np.log10(1e20*X_train[:,:,148]**8.5 + 1e-10) + 1\n",
    "X_train[:,:,149] = 0.1 * np.log10(1e20*X_train[:,:,149]**9 + 1e-10) + 1\n",
    "X_train[:,:,150] = 0.1 * np.log10(1e20*X_train[:,:,150]**7.5 + 1e-10) + 1\n",
    "X_train[:,:,151] = 0.1 * np.log10(1e20*X_train[:,:,151]**12 + 1e-10) + 1\n",
    "X_train[:,:,152] = 0.1 * np.log10(1e20*X_train[:,:,152]**20 + 1e-10) + 1\n",
    "X_train[:,:,153] = 0.1 * np.log10(1e20*X_train[:,:,153]**11 + 1e-10) + 1\n",
    "X_train[:,:,154] = 0.1 * np.log10(1e20*X_train[:,:,154]**20 + 1e-10) + 1\n",
    "X_train[:,:,155] = 0.1 * np.log10(1e20*X_train[:,:,155]**16 + 1e-10) + 1\n",
    "X_train[:,:,156] = 0.1 * np.log10(1e20*X_train[:,:,156]**10 + 1e-10) + 1\n",
    "X_train[:,:,157] = 0.1 * np.log10(1e20*X_train[:,:,157]**11 + 1e-10) + 1\n",
    "X_train[:,:,158] = 0.1 * np.log10(1e20*X_train[:,:,158]**9 + 1e-10) + 1\n",
    "X_train[:,:,159] = 0.1 * np.log10(1e20*X_train[:,:,159]**3.5 + 1e-10) + 1\n",
    "X_train[:,:,160] = 0.1 * np.log10(1e20*X_train[:,:,160]**11 + 1e-10) + 1\n",
    "X_train[:,:,166] = 0.1 * np.log10(1e20*X_train[:,:,166]**11 + 1e-10) + 1\n",
    "X_train[:,:,168] = 0.1 * np.log10(1e20*X_train[:,:,168]**14 + 1e-10) + 1\n",
    "X_train[:,:,173] = 0.1 * np.log10(1e30*X_train[:,:,173]**7 + 1e-10) + 1\n",
    "X_train[:,:,174] = 0.1 * np.log10(1e30*X_train[:,:,174]**4 + 1e-10) + 1\n",
    "X_train[:,:,178] = 0.1 * np.log10(1e30*X_train[:,:,178]**4 + 1e-10) + 1\n",
    "X_train[:,:,179] = 0.1 * np.log10(1e30*X_train[:,:,179]**4 + 1e-10) + 1\n",
    "X_train[:,:,181] = 0.1 * np.log10(1e30*X_train[:,:,181]**7 + 1e-10) + 1\n",
    "X_train[:,:,182] = 0.1 * np.log10(1e30*X_train[:,:,182]**2 + 1e-10) + 1\n",
    "X_train[:,:,183] = 0.1 * np.log10(1e30*X_train[:,:,183]**2 + 1e-10) + 1\n",
    "X_train[:,:,185] = 0.1 * np.log10(1e30*X_train[:,:,185]**2 + 1e-10) + 1\n",
    "X_train[:,:,186] = 0.1 * np.log10(1e30*X_train[:,:,186]**2 + 1e-10) + 1\n",
    "X_train[:,:,187] = 0.1 * np.log10(1e30*X_train[:,:,187]**2 + 1e-10) + 1\n",
    "X_train[:,:,190] = 0.1 * np.log10(1e10*X_train[:,:,190]**4 + 1e-10) + 1\n",
    "X_train[:,:,191] = 0.1 * np.log10(1e20*X_train[:,:,191]**6 + 1e-10) + 1\n",
    "X_train[:,:,192] = 0.1 * np.log10(1e20*X_train[:,:,192]**4 + 1e-10) + 1\n",
    "X_train[:,:,195] = 0.1 * np.log10(1e20*X_train[:,:,195]**4 + 1e-10) + 1\n",
    "X_train[:,:,196] = 0.1 * np.log10(1e20*X_train[:,:,196]**6 + 1e-10) + 1\n",
    "X_train[:,:,197] = 0.1 * np.log10(1e20*X_train[:,:,197]**4 + 1e-10) + 1\n",
    "X_train[:,:,198] = 0.1 * np.log10(1e20*X_train[:,:,198]**6 + 1e-10) + 1\n",
    "X_train[:,:,200] = 0.1 * np.log10(1e20*X_train[:,:,200]**6 + 1e-10) + 1\n",
    "X_train[:,:,201] = 0.1 * np.log10(1e20*X_train[:,:,201]**5 + 1e-10) + 1\n",
    "X_train[:,:,202] = 0.1 * np.log10(1e20*X_train[:,:,202]**5 + 1e-10) + 1\n",
    "X_train[:,:,203] = 0.1 * np.log10(1e20*X_train[:,:,203]**5 + 1e-10) + 1\n",
    "X_train[:,:,204] = 0.1 * np.log10(1e30*X_train[:,:,204]**3 + 1e-10) + 1\n",
    "X_train[:,:,205] = 0.1 * np.log10(1e30*X_train[:,:,205]**3 + 1e-10) + 1\n",
    "X_train[:,:,206] = 0.1 * np.log10(1e20*X_train[:,:,206]**7 + 1e-10) + 1\n",
    "X_train[:,:,207] = 0.1 * np.log10(1e30*X_train[:,:,207]**2 + 1e-10) + 1\n",
    "X_train[:,:,208] = 0.1 * np.log10(1e30*X_train[:,:,208]**2 + 1e-10) + 1\n",
    "X_train[:,:,209] = 0.1 * np.log10(1e30*X_train[:,:,209]**2 + 1e-10) + 1\n",
    "X_train[:,:,213] = 0.1 * np.log10(1e30*X_train[:,:,213]**2 + 1e-10) + 1\n",
    "X_train[:,:,214] = 0.1 * np.log10(1e7*X_train[:,:,214]**2 + 1e-10) + 1\n",
    "X_train[:,:,215] = 0.1 * np.log10(1e15*X_train[:,:,215]**2 + 1e-10) + 1\n",
    "X_train[:,:,216] = 0.1 * np.log10(1e15*X_train[:,:,216]**2 + 1e-10) + 1\n",
    "X_train[:,:,217] = 0.1 * np.log10(1e15*X_train[:,:,217]**2 + 1e-10) + 1\n",
    "X_train[:,:,218] = 0.1 * np.log10(1e15*X_train[:,:,218]**2 + 1e-10) + 1\n",
    "X_train[:,:,220] = 0.1 * np.log10(1e15*X_train[:,:,220]**2 + 1e-10) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Numpy tensor into PyTorch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "train_data = []\n",
    "for i in range(len(X_train)):\n",
    "    train_data.append((X_train[i], y_train[i]))\n",
    "    \n",
    "test_data = []\n",
    "for i in range(len(X_test)):\n",
    "    test_data.append((X_test[i], y_test[i]))\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "### Classifier / Focal Loss initialization\n",
    "Notice that the dimensions per layer are recommended to set to a smaller value due to the risk of overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = ConvNet(fc_dims=[128, 64], in_dim=X_train.shape[2], out_dim=2).cuda()\n",
    "\n",
    "focal_loss = FocalLoss2(alpha, gamma_pos, gamma_neg)\n",
    "if optim_type == \"adam\":\n",
    "    optim_clsfr = optim.Adam(filter(lambda p: p.requires_grad, classifier.parameters()), \n",
    "                             lr=learn_rate)\n",
    "else:\n",
    "    optim_clsfr = RangerLars(filter(lambda p: p.requires_grad, classifier.parameters()),\n",
    "                             lr=learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Parameter Setting ----------------------------------------------------------------------')\n",
    "print('Model = VAT + Conv1D')\n",
    "print('conv1d use activation = {}'.format(True))\n",
    "print('graph_emdeding = Heter_AutoEncoder')\n",
    "print('alpha = {}'.format(alpha))\n",
    "print('gamma_pos = {}'.format(gamma_pos))\n",
    "print('gamma_neg = {}'.format(gamma_neg))\n",
    "print('learn_rate = {}'.format(learn_rate))\n",
    "print('train_batch_size = {}'.format(train_batch_size))\n",
    "print('test_batch_size = {}'.format(test_batch_size))\n",
    "print('max_epochs = {}'.format(max_epochs))\n",
    "print('vat_xi = {}'.format(vat_xi))\n",
    "print('vat_eps_pos = {}'.format(vat_eps_pos))\n",
    "print('vat_eps_neg = {}'.format(vat_eps_neg))\n",
    "print('vat_ip = {}'.format(vat_ip))\n",
    "print('optim_type = {}'.format(optim_type))\n",
    "print('weight_decay = {}'.format(weight_decay))\n",
    "print('lambda_l1 = {}'.format(lambda_l1))\n",
    "print('\\n')\n",
    "\n",
    "train_history_loss = []\n",
    "train_history_auc = []\n",
    "max_f1score = 0\n",
    "for epoch in range(max_epochs):\n",
    "    print('Epoch {} -------------------------------------------------------------------------'.format(epoch))\n",
    "    \n",
    "    #\n",
    "    # Training\n",
    "    # ------------------------------------------------------------------------------------\n",
    "    classifier.train()\n",
    "    label_train, pred_y_train, clsf_loss_train, clsf_loss_pos_train, \\\n",
    "        clsf_loss_neg_train, vat_loss_train = train(epoch, train_dataloader, \n",
    "                                                    clip_grad_norm=grad_clip, lambda_l1=lambda_l1)\n",
    "    \n",
    "    auc_train = roc_auc_score(label_train, pred_y_train)\n",
    "    train_history_loss.append(clsf_loss_train)\n",
    "    train_history_auc.append(auc_train)\n",
    "    print('    Training => auc: {:.6f}, clsf_pos: {}, clsf_neg: {}, vat_loss: {}'.\n",
    "          format(auc_train, clsf_loss_pos_train, clsf_loss_neg_train, vat_loss_train))\n",
    "    thres = np.min(pred_y_train[label_train==1]) - 1e-6\n",
    "    print(\"                Threshold is set to {}\".format(thres))\n",
    "    print(\"                Min. Probailities on train is {}\".format(np.min(pred_y_train)))\n",
    "    \n",
    "    y_predict_bin = np.array(pred_y_train > thres, dtype=int)\n",
    "    prec_train, recall_train, f1_train = evaluate(label_train, y_predict_bin)\n",
    "    print('                prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}'.\n",
    "          format(prec_train, recall_train, f1_train))\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        #\n",
    "        # Testing\n",
    "        # ------------------------------------------------------------------------------------        \n",
    "        with torch.no_grad():\n",
    "            classifier.eval()\n",
    "            label_test, pred_y_test, clsf_loss_test, clsf_loss_pos_test, clsf_loss_neg_test = infer(test_dataloader)    \n",
    "        \n",
    "        auc = roc_auc_score(label_test, pred_y_test)\n",
    "        \n",
    "        print(\"            Min. Probailities on test set with label 1: {}\".format(np.min(pred_y_test[label_test==1])))\n",
    "        print(\"            Min. Probailities on test set             : {}\".format(np.min(pred_y_test)))\n",
    "        \n",
    "        y_predict_bin = np.array(pred_y_test > thres, dtype=int)\n",
    "        prec, recall_bytrain, f1_bytrain = evaluate(label_test, y_predict_bin)\n",
    "        print('    Testing by train ==> auc: {:.6f}, prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}, clsf_loss: {}'.\n",
    "              format(auc, prec, recall_bytrain, f1_bytrain, clsf_loss_test))\n",
    "        \n",
    "        y_predict_bin = np.array(pred_y_test >= np.min(pred_y_test[label_test==1]), dtype=int)\n",
    "        prec, recall_bytest, f1_bytest = evaluate(label_test, y_predict_bin)\n",
    "        print('    Testing by test ==> auc: {:.6f}, prec: {:.4f}, rec: {:.4f}, F1score: {:.4f}, clsf_loss: {}'.\n",
    "              format(auc, prec, recall_bytest, f1_bytest, clsf_loss_test))\n",
    "        \n",
    "        if clsf_loss_pos_train < 0.005 and f1_bytrain > max_f1score and recall_bytrain == 1:\n",
    "            max_f1score = f1_bytrain\n",
    "            torch.save({'epoch': epoch,\n",
    "                        'model_state_dict': classifier.state_dict(),\n",
    "                        'optimizer_state_dict': optim_clsfr.state_dict(),\n",
    "                        'loss': focal_loss,\n",
    "                       }, \n",
    "                       '../../user_data/CloudMile/data/saved_models/' + \\\n",
    "                       'FeatEmbed-Conv1Dsmall_v2_eps{}{}_focal{}{}_{}_lr{}_epoch{}_Part2_{}'.\n",
    "                       format(int(-math.log10(vat_eps_pos)), \n",
    "                              int(-math.log10(vat_eps_neg)), \n",
    "                              gamma_pos, gamma_neg, \n",
    "                              optim_type, int(-math.log10(learn_rate)), max_epochs, f1_bytrain))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PATH = '{model path}'\n",
    "performance(PATH, 'FeatEmbed-Golden-LastSoftmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 recall@N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recallN(df, N):\n",
    "    return df.head(N)['label'].sum()/ df['label'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>label</th>\n",
       "      <th>annouce</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.523318</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>0.523318</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>0.523318</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>0.523318</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>0.523318</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         score  label  annouce\n",
       "355   0.523318    1.0      0.0\n",
       "1057  0.523318    1.0      0.0\n",
       "2034  0.523318    1.0      0.0\n",
       "2035  0.523318    1.0      0.0\n",
       "1091  0.523318    1.0      0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/matthewliu/AML-CTBC/Feat-ts-Training_result.csv')\n",
    "df = df.sort_values('score', ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recallN(df, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
